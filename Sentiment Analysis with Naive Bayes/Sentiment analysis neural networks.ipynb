{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unknown words are removed (maybe affects the sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn.functional as f\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding_file = '/Users/sougata-8718/Documents/NLP/Embeddings/glove6b/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = open(glove_embedding_file, \"r+\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = contents.split(\"\\n\")\n",
    "contents = [ [x.split(\" \")[0], np.array(x.split(\" \")[1::])] for x in contents ][0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "embeddings = []\n",
    "\n",
    "for word, embedding in contents:\n",
    "    words.append(word)\n",
    "    embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idx_map = dict(zip(words, np.arange(len(words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_map = dict(zip(words, embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = open(\"/Users/sougata-8718/Downloads/amazon_reviews_train.csv\",\"r+\")\n",
    "test = open(\"/Users/sougata-8718/Downloads/amazon_reviews_test.csv\",\"r+\")\n",
    "train = train.read()\n",
    "test = test.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def messy_text_to_df(text):\n",
    "    documents = text.split(\"\\n\")\n",
    "    df = pd.DataFrame()\n",
    "    data = []\n",
    "    labels = []\n",
    "    for document in documents:\n",
    "        labels.append(document.split(\"\\t\",1)[0])\n",
    "        text = document.split('\\t')[1]\n",
    "        data.append(text)\n",
    "    labels = np.array(labels)\n",
    "    labels[np.where(labels=='__label__2')] = \"Positive\"\n",
    "    labels[np.where(labels=='__label__1')] = \"Negative\"\n",
    "    df[\"Data\"] = data\n",
    "    df[\"Label\"] = labels\n",
    "    \n",
    "    return df\n",
    "\n",
    "def remove_punctuation_and_numbers(text,replacements):\n",
    "    for key,value in replacements.items():\n",
    "        text = text.replace(key,value)\n",
    "    text = text.translate(str.maketrans('','',';\"#$%&\\'()*+/<=>?@[\\\\]^_`{|}~0123456789')).translate(str.maketrans('!.-:,','     '))\n",
    "    return text\n",
    "def remove_non_words(data,replacements):\n",
    "    res = data.apply(lambda x: remove_punctuation_and_numbers(x,replacements))\n",
    "    return res\n",
    "\n",
    "\n",
    "def remove_words_single(string,words_to_be_removed):\n",
    "    words = nltk.word_tokenize(string)\n",
    "    filtered_words = []\n",
    "    for i in range(len(words)):\n",
    "        if words[i] not in words_to_be_removed:\n",
    "            filtered_words.append(words[i])\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def remove_words(data,words_to_be_removed):\n",
    "    res = data.apply(lambda x : remove_words_single(x,words_to_be_removed))\n",
    "    return res\n",
    "    \n",
    "    for text,label in documents:\n",
    "        labels.append(document.split(\"\\t\",1)[0])\n",
    "        text = document.split('\\t')[1]\n",
    "        for key,value in replacements.items():\n",
    "            text = text.replace(key,value)\n",
    "            text = text.translate(str.maketrans('','',';\"#$%&\\'()*+/<=>?@[\\\\]^_`{|}~0123456789')).translate(str.maketrans('!.-:,','     '))\n",
    "        words = nltk.word_tokenize(text)\n",
    "        filtered_words = []\n",
    "        for i in range(len(words)):\n",
    "            if words[i] not in words_to_be_removed:\n",
    "                filtered_words.append(stemmer.stem(words[i]))\n",
    "        \n",
    "        res = ' '.join(filtered_words)\n",
    "        data.append(res)\n",
    "    labels = np.array(labels)\n",
    "    labels[np.where(labels=='__label__2')] = \"Positive\"\n",
    "    labels[np.where(labels=='__label__1')] = \"Negative\"\n",
    "    return data,labels\n",
    "\n",
    "def stem_single_string(string,nltkstemmer):\n",
    "    words = nltk.word_tokenize(string)\n",
    "    stemmed_list = []\n",
    "    for word in words:\n",
    "        stemmed_list.append(nltkstemmer.stem(word))\n",
    "    return ' '.join(stemmed_list)\n",
    "    \n",
    "\n",
    "def stem(data):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    res = data.apply(lambda x : stem_single_string(x,stemmer))\n",
    "    return res\n",
    "\n",
    "def find_rare_words(data,max_frequency=4):\n",
    "    \n",
    "    vectoriser = get_vectorizer(data)\n",
    "    \n",
    "    \n",
    "    temp = ' '.join(data)\n",
    "    frequencies = (nltk.FreqDist(nltk.word_tokenize(temp)))\n",
    "    \n",
    "    fs = np.array(frequencies.most_common())\n",
    "    fs = pd.DataFrame(fs)\n",
    "    fs.columns = [\"word\",\"count\"]\n",
    "    fs[\"freq\"] = fs[\"count\"].astype(int)\n",
    "    fs = fs.drop(\"count\",axis=1)\n",
    "    \n",
    "    rare_words = list(fs[fs[\"freq\"]<=max_frequency][\"word\"])\n",
    "    \n",
    "    return rare_words\n",
    "\n",
    "def get_vectorizer(data,vectorizer=\"CountVectorizer\"):\n",
    "    \n",
    "    if vectorizer == \"TFIDF\":\n",
    "        tfidf = TfidfVectorizer()\n",
    "        tfidf.fit(data)\n",
    "        print(\"TF-IDF Vectorizer\")\n",
    "        return tfidf\n",
    "    cv = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "    cv.fit(data)\n",
    "    print(\"Count Vectorizer\")\n",
    "    return cv\n",
    "\n",
    "def vectorize_data(data,vectorizer=\"CountVectorizer\"):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "    \n",
    "    if vectorizer == \"TFIDF\":\n",
    "        tfidf = TfidfVectorizer()\n",
    "        tfidf.fit(data)\n",
    "        print(\"TF-IDF Vectorizer\")\n",
    "        return tfidf.transform(data).toarray()\n",
    "    cv = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "    cv.fit(data)\n",
    "    print(\"Count Vectorizer\")\n",
    "    return cv.transform(data).toarray()\n",
    "\n",
    "def preprocess(data):\n",
    "    data = messy_text_to_df(data)\n",
    "    data[\"Data\"] = remove_non_words(data[\"Data\"],replacements)\n",
    "    data[\"Data\"] = data[\"Data\"].apply(lambda x : x.lower())\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = {\"can't\" : 'can not',\"shan't\":'shall not',\"won't\":'will not',\"'ve\" : \" have\", \"'d\" : \" would\", \"'m\" : \" am\", \"'ll\" : \" will\", \"'s\" : \"\", \"n't\" : \" not\",\"'re\" : \"are\"}\n",
    "train = preprocess(train)\n",
    "test = preprocess(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['0.418', '0.24968', '-0.41242', '0.1217', '0.34527', '-0.044457',\n",
       "        '-0.49688', '-0.17862', '-0.00066023', '-0.6566', '0.27843',\n",
       "        '-0.14767', '-0.55677', '0.14658', '-0.0095095', '0.011658',\n",
       "        '0.10204', '-0.12792', '-0.8443', '-0.12181', '-0.016801',\n",
       "        '-0.33279', '-0.1552', '-0.23131', '-0.19181', '-1.8823',\n",
       "        '-0.76746', '0.099051', '-0.42125', '-0.19526', '4.0071',\n",
       "        '-0.18594', '-0.52287', '-0.31681', '0.00059213', '0.0074449',\n",
       "        '0.17778', '-0.15897', '0.012041', '-0.054223', '-0.29871',\n",
       "        '-0.15749', '-0.34758', '-0.045637', '-0.44251', '0.18785',\n",
       "        '0.0027849', '-0.18411', '-0.11514', '-0.78581'], dtype='<U11'),\n",
       " array([-0.0482406 ,  0.17023812, -0.00056398,  0.06081297,  0.01771576,\n",
       "         0.14994409, -0.17120126, -0.12530452,  0.031613  , -0.09040955,\n",
       "        -0.09735695,  0.1414364 ,  0.11935937,  0.01789004,  0.07101575,\n",
       "         0.08698796,  0.00429509,  0.18495536,  0.08105742, -0.15570799,\n",
       "        -0.02836831,  0.09633531,  0.11068859, -0.1398255 ,  0.30996252,\n",
       "         0.23012362, -0.06257258,  0.09758483,  0.020786  ,  0.14691186,\n",
       "         0.05356768,  0.04832678, -0.06023957, -0.13123241,  0.05147257,\n",
       "         0.23215136, -0.09397164,  0.05118327,  0.24095792, -0.12686159,\n",
       "         0.16493841, -0.02342633,  0.16958821, -0.13296194, -0.04026461,\n",
       "        -0.06858703,  0.13891426,  0.04133224,  0.0327858 ,  0.26507192]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding_map.get(\"the\"), np.random.randn(50)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_summed_vector(sentence):\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    word_embeddings = [embeddings[word_idx_map.get(word)] if word_idx_map.get(word)!=None else np.zeros((50,)) for word in words]\n",
    "    word_embeddings = np.array(word_embeddings).astype(np.float)\n",
    "    \n",
    "    return np.sum(word_embeddings, axis = 0)\n",
    "\n",
    "def sentence_to_sequence_vectors(sentence):\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    words = [word if word_idx_map.get(word)!= None else '' for word in words]\n",
    "    words = nltk.word_tokenize(' '.join(words))\n",
    "    word_embeddings = [embeddings[word_idx_map.get(word)] for word in words]\n",
    "    word_embeddings = torch.Tensor(np.array(word_embeddings).astype(np.float))\n",
    "    #word_embeddings = word_embeddings.resize(1, word_embeddings.shape[0], word_embeddings.shape[1])\n",
    "    \n",
    "    return word_embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.Tensor(train[\"Data\"].apply(lambda x : sentence_to_summed_vector(x)))\n",
    "X_test = torch.Tensor(test[\"Data\"].apply(lambda x : sentence_to_summed_vector(x)))\n",
    "Y_train = torch.Tensor([[1, 0] if x == \"Positive\" else [0, 1] for x in train[\"Label\"]])\n",
    "Y_test = torch.Tensor([[1, 0] if x == \"Positive\" else [0, 1] for x in test[\"Label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "        self.hidden1 = torch.nn.Linear(input_shape, 50)\n",
    "        self.hidden2 = torch.nn.Linear(50, 25)\n",
    "        self.output = torch.nn.Linear(25, 2)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr = 0.001)\n",
    "        pass\n",
    "    \n",
    "    def train(self, X_train, Y_train, X_test, Y_test, no_of_iterations):\n",
    "        for i in range(no_of_iterations):\n",
    "            state = self.hidden1(X_train)\n",
    "            state = f.tanh(state)\n",
    "            state = self.hidden2(state)\n",
    "            state = f.tanh(state)\n",
    "            state = self.output(state)\n",
    "            state = f.softmax(state, dim = -1)\n",
    "\n",
    "            loss = - (Y_train * torch.log(state))\n",
    "            loss = torch.sum(loss)/len(X_train)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            train_acc = accuracy_score(y_pred = torch.argmax(state.detach(), 1), y_true = torch.argmax(Y_train.detach(),1))\n",
    "            train_f1 = f1_score(y_pred = torch.argmax(state.detach(), -1,), y_true = torch.argmax(Y_train.detach(),1), average = 'macro')\n",
    "            #print(torch.argmax(state.detach(), 1)[0:100],torch.argmax(Y_train.detach(),1)[0:100])\n",
    "            test_preds = self.forward(X_test)\n",
    "            test_acc = accuracy_score(y_pred = torch.argmax(test_preds.detach(), 1), y_true = torch.argmax(Y_test.detach(),1))\n",
    "            test_f1 = f1_score(y_pred = torch.argmax(test_preds.detach(), -1,), y_true = torch.argmax(Y_test.detach(),1), average = 'macro')\n",
    "            print(\"iteration\", i+1, \"loss\", loss.item(), \"train acc/f1\", round(train_acc, 3), \"/\",round(train_f1,3), \"test acc/f1\", round(test_acc, 3), \"/\", round(test_f1,3),\"\\n\")\n",
    "            pass\n",
    "        pass\n",
    "    \n",
    "    def forward(self, X):\n",
    "        state = self.hidden1(X)\n",
    "        state = f.tanh(state)\n",
    "        state = self.hidden2(state)\n",
    "        state = f.tanh(state)\n",
    "        state = self.output(state)\n",
    "        state = f.softmax(state, dim = -1)\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 loss 0.695752739906311 train acc/f1 0.519 / 0.48 test acc/f1 0.562 / 0.562 \n",
      "\n",
      "iteration 2 loss 0.6916160583496094 train acc/f1 0.523 / 0.519 test acc/f1 0.577 / 0.576 \n",
      "\n",
      "iteration 3 loss 0.6871929168701172 train acc/f1 0.548 / 0.548 test acc/f1 0.578 / 0.56 \n",
      "\n",
      "iteration 4 loss 0.6832969784736633 train acc/f1 0.558 / 0.552 test acc/f1 0.56 / 0.527 \n",
      "\n",
      "iteration 5 loss 0.6806715130805969 train acc/f1 0.566 / 0.55 test acc/f1 0.576 / 0.55 \n",
      "\n",
      "iteration 6 loss 0.6773461103439331 train acc/f1 0.576 / 0.564 test acc/f1 0.599 / 0.587 \n",
      "\n",
      "iteration 7 loss 0.6734445691108704 train acc/f1 0.589 / 0.584 test acc/f1 0.623 / 0.62 \n",
      "\n",
      "iteration 8 loss 0.669662356376648 train acc/f1 0.597 / 0.597 test acc/f1 0.621 / 0.62 \n",
      "\n",
      "iteration 9 loss 0.6660543084144592 train acc/f1 0.607 / 0.607 test acc/f1 0.62 / 0.619 \n",
      "\n",
      "iteration 10 loss 0.6619347929954529 train acc/f1 0.617 / 0.617 test acc/f1 0.63 / 0.627 \n",
      "\n",
      "iteration 11 loss 0.6575955748558044 train acc/f1 0.624 / 0.624 test acc/f1 0.628 / 0.623 \n",
      "\n",
      "iteration 12 loss 0.6534050703048706 train acc/f1 0.63 / 0.629 test acc/f1 0.638 / 0.633 \n",
      "\n",
      "iteration 13 loss 0.6491103768348694 train acc/f1 0.635 / 0.633 test acc/f1 0.643 / 0.64 \n",
      "\n",
      "iteration 14 loss 0.6447763442993164 train acc/f1 0.641 / 0.639 test acc/f1 0.652 / 0.65 \n",
      "\n",
      "iteration 15 loss 0.6407711505889893 train acc/f1 0.645 / 0.645 test acc/f1 0.669 / 0.668 \n",
      "\n",
      "iteration 16 loss 0.6371012926101685 train acc/f1 0.649 / 0.649 test acc/f1 0.67 / 0.669 \n",
      "\n",
      "iteration 17 loss 0.6333805322647095 train acc/f1 0.654 / 0.654 test acc/f1 0.676 / 0.675 \n",
      "\n",
      "iteration 18 loss 0.629447340965271 train acc/f1 0.658 / 0.658 test acc/f1 0.678 / 0.677 \n",
      "\n",
      "iteration 19 loss 0.625434398651123 train acc/f1 0.663 / 0.663 test acc/f1 0.685 / 0.683 \n",
      "\n",
      "iteration 20 loss 0.621411144733429 train acc/f1 0.667 / 0.667 test acc/f1 0.689 / 0.688 \n",
      "\n",
      "iteration 21 loss 0.6174429059028625 train acc/f1 0.672 / 0.671 test acc/f1 0.687 / 0.686 \n",
      "\n",
      "iteration 22 loss 0.6137115359306335 train acc/f1 0.673 / 0.673 test acc/f1 0.687 / 0.687 \n",
      "\n",
      "iteration 23 loss 0.6098302006721497 train acc/f1 0.679 / 0.679 test acc/f1 0.694 / 0.693 \n",
      "\n",
      "iteration 24 loss 0.6056857705116272 train acc/f1 0.683 / 0.683 test acc/f1 0.696 / 0.695 \n",
      "\n",
      "iteration 25 loss 0.6015335917472839 train acc/f1 0.688 / 0.688 test acc/f1 0.697 / 0.696 \n",
      "\n",
      "iteration 26 loss 0.5972762703895569 train acc/f1 0.692 / 0.691 test acc/f1 0.699 / 0.698 \n",
      "\n",
      "iteration 27 loss 0.5930007100105286 train acc/f1 0.693 / 0.693 test acc/f1 0.706 / 0.706 \n",
      "\n",
      "iteration 28 loss 0.5888076424598694 train acc/f1 0.695 / 0.695 test acc/f1 0.706 / 0.706 \n",
      "\n",
      "iteration 29 loss 0.5846719145774841 train acc/f1 0.7 / 0.7 test acc/f1 0.708 / 0.708 \n",
      "\n",
      "iteration 30 loss 0.5805972814559937 train acc/f1 0.702 / 0.702 test acc/f1 0.703 / 0.703 \n",
      "\n",
      "iteration 31 loss 0.5764287114143372 train acc/f1 0.704 / 0.704 test acc/f1 0.706 / 0.706 \n",
      "\n",
      "iteration 32 loss 0.5721653699874878 train acc/f1 0.707 / 0.707 test acc/f1 0.716 / 0.716 \n",
      "\n",
      "iteration 33 loss 0.5679717659950256 train acc/f1 0.711 / 0.711 test acc/f1 0.717 / 0.717 \n",
      "\n",
      "iteration 34 loss 0.5637548565864563 train acc/f1 0.714 / 0.714 test acc/f1 0.72 / 0.72 \n",
      "\n",
      "iteration 35 loss 0.5595998764038086 train acc/f1 0.715 / 0.715 test acc/f1 0.723 / 0.723 \n",
      "\n",
      "iteration 36 loss 0.5555199980735779 train acc/f1 0.718 / 0.718 test acc/f1 0.725 / 0.725 \n",
      "\n",
      "iteration 37 loss 0.5514503717422485 train acc/f1 0.721 / 0.72 test acc/f1 0.727 / 0.727 \n",
      "\n",
      "iteration 38 loss 0.5473902821540833 train acc/f1 0.724 / 0.724 test acc/f1 0.732 / 0.732 \n",
      "\n",
      "iteration 39 loss 0.5433441400527954 train acc/f1 0.728 / 0.727 test acc/f1 0.735 / 0.735 \n",
      "\n",
      "iteration 40 loss 0.5393707156181335 train acc/f1 0.73 / 0.73 test acc/f1 0.738 / 0.738 \n",
      "\n",
      "iteration 41 loss 0.5355584025382996 train acc/f1 0.733 / 0.733 test acc/f1 0.745 / 0.745 \n",
      "\n",
      "iteration 42 loss 0.5319451093673706 train acc/f1 0.735 / 0.735 test acc/f1 0.75 / 0.75 \n",
      "\n",
      "iteration 43 loss 0.5285872220993042 train acc/f1 0.738 / 0.738 test acc/f1 0.752 / 0.752 \n",
      "\n",
      "iteration 44 loss 0.5253135561943054 train acc/f1 0.74 / 0.74 test acc/f1 0.754 / 0.754 \n",
      "\n",
      "iteration 45 loss 0.5221748948097229 train acc/f1 0.742 / 0.742 test acc/f1 0.757 / 0.757 \n",
      "\n",
      "iteration 46 loss 0.5192790627479553 train acc/f1 0.745 / 0.745 test acc/f1 0.76 / 0.76 \n",
      "\n",
      "iteration 47 loss 0.5167419910430908 train acc/f1 0.746 / 0.746 test acc/f1 0.765 / 0.765 \n",
      "\n",
      "iteration 48 loss 0.5145807266235352 train acc/f1 0.746 / 0.746 test acc/f1 0.765 / 0.765 \n",
      "\n",
      "iteration 49 loss 0.5123012065887451 train acc/f1 0.747 / 0.747 test acc/f1 0.766 / 0.766 \n",
      "\n",
      "iteration 50 loss 0.5100531578063965 train acc/f1 0.748 / 0.748 test acc/f1 0.766 / 0.766 \n",
      "\n",
      "iteration 51 loss 0.5080738663673401 train acc/f1 0.752 / 0.752 test acc/f1 0.768 / 0.768 \n",
      "\n",
      "iteration 52 loss 0.5064528584480286 train acc/f1 0.753 / 0.753 test acc/f1 0.768 / 0.768 \n",
      "\n",
      "iteration 53 loss 0.5048734545707703 train acc/f1 0.754 / 0.754 test acc/f1 0.767 / 0.767 \n",
      "\n",
      "iteration 54 loss 0.5030990242958069 train acc/f1 0.754 / 0.754 test acc/f1 0.764 / 0.764 \n",
      "\n",
      "iteration 55 loss 0.5013219118118286 train acc/f1 0.755 / 0.755 test acc/f1 0.764 / 0.764 \n",
      "\n",
      "iteration 56 loss 0.4998090863227844 train acc/f1 0.757 / 0.757 test acc/f1 0.764 / 0.764 \n",
      "\n",
      "iteration 57 loss 0.4984830617904663 train acc/f1 0.757 / 0.757 test acc/f1 0.766 / 0.766 \n",
      "\n",
      "iteration 58 loss 0.49709418416023254 train acc/f1 0.759 / 0.758 test acc/f1 0.769 / 0.769 \n",
      "\n",
      "iteration 59 loss 0.49563920497894287 train acc/f1 0.761 / 0.761 test acc/f1 0.767 / 0.767 \n",
      "\n",
      "iteration 60 loss 0.49425119161605835 train acc/f1 0.764 / 0.764 test acc/f1 0.769 / 0.769 \n",
      "\n",
      "iteration 61 loss 0.49283653497695923 train acc/f1 0.765 / 0.765 test acc/f1 0.769 / 0.769 \n",
      "\n",
      "iteration 62 loss 0.49148741364479065 train acc/f1 0.763 / 0.763 test acc/f1 0.769 / 0.769 \n",
      "\n",
      "iteration 63 loss 0.4904153048992157 train acc/f1 0.766 / 0.766 test acc/f1 0.765 / 0.765 \n",
      "\n",
      "iteration 64 loss 0.48943766951560974 train acc/f1 0.764 / 0.764 test acc/f1 0.769 / 0.769 \n",
      "\n",
      "iteration 65 loss 0.4884394109249115 train acc/f1 0.768 / 0.767 test acc/f1 0.764 / 0.764 \n",
      "\n",
      "iteration 66 loss 0.48727360367774963 train acc/f1 0.766 / 0.766 test acc/f1 0.772 / 0.772 \n",
      "\n",
      "iteration 67 loss 0.48586371541023254 train acc/f1 0.768 / 0.768 test acc/f1 0.769 / 0.769 \n",
      "\n",
      "iteration 68 loss 0.48468825221061707 train acc/f1 0.77 / 0.77 test acc/f1 0.768 / 0.768 \n",
      "\n",
      "iteration 69 loss 0.48392030596733093 train acc/f1 0.769 / 0.769 test acc/f1 0.767 / 0.767 \n",
      "\n",
      "iteration 70 loss 0.4830847382545471 train acc/f1 0.77 / 0.77 test acc/f1 0.768 / 0.768 \n",
      "\n",
      "iteration 71 loss 0.481889933347702 train acc/f1 0.769 / 0.769 test acc/f1 0.767 / 0.767 \n",
      "\n",
      "iteration 72 loss 0.48078736662864685 train acc/f1 0.771 / 0.771 test acc/f1 0.764 / 0.764 \n",
      "\n",
      "iteration 73 loss 0.4800460636615753 train acc/f1 0.771 / 0.771 test acc/f1 0.768 / 0.768 \n",
      "\n",
      "iteration 74 loss 0.4793182909488678 train acc/f1 0.77 / 0.77 test acc/f1 0.764 / 0.764 \n",
      "\n",
      "iteration 75 loss 0.47834083437919617 train acc/f1 0.773 / 0.772 test acc/f1 0.765 / 0.765 \n",
      "\n",
      "iteration 76 loss 0.47735854983329773 train acc/f1 0.773 / 0.773 test acc/f1 0.767 / 0.767 \n",
      "\n",
      "iteration 77 loss 0.47662365436553955 train acc/f1 0.773 / 0.773 test acc/f1 0.766 / 0.766 \n",
      "\n",
      "iteration 78 loss 0.47596728801727295 train acc/f1 0.775 / 0.774 test acc/f1 0.77 / 0.77 \n",
      "\n",
      "iteration 79 loss 0.4751605987548828 train acc/f1 0.773 / 0.773 test acc/f1 0.767 / 0.767 \n",
      "\n",
      "iteration 80 loss 0.4742948114871979 train acc/f1 0.776 / 0.776 test acc/f1 0.764 / 0.764 \n",
      "\n",
      "iteration 81 loss 0.47356608510017395 train acc/f1 0.776 / 0.776 test acc/f1 0.768 / 0.768 \n",
      "\n",
      "iteration 82 loss 0.47295308113098145 train acc/f1 0.775 / 0.775 test acc/f1 0.763 / 0.763 \n",
      "\n",
      "iteration 83 loss 0.47228172421455383 train acc/f1 0.777 / 0.777 test acc/f1 0.766 / 0.766 \n",
      "\n",
      "iteration 84 loss 0.47151806950569153 train acc/f1 0.777 / 0.777 test acc/f1 0.765 / 0.765 \n",
      "\n",
      "iteration 85 loss 0.47078606486320496 train acc/f1 0.778 / 0.778 test acc/f1 0.763 / 0.763 \n",
      "\n",
      "iteration 86 loss 0.4701378047466278 train acc/f1 0.779 / 0.779 test acc/f1 0.766 / 0.766 \n",
      "\n",
      "iteration 87 loss 0.46953874826431274 train acc/f1 0.779 / 0.779 test acc/f1 0.765 / 0.765 \n",
      "\n",
      "iteration 88 loss 0.4689353406429291 train acc/f1 0.779 / 0.779 test acc/f1 0.767 / 0.767 \n",
      "\n",
      "iteration 89 loss 0.4682820737361908 train acc/f1 0.779 / 0.779 test acc/f1 0.769 / 0.769 \n",
      "\n",
      "iteration 90 loss 0.46758413314819336 train acc/f1 0.78 / 0.78 test acc/f1 0.766 / 0.766 \n",
      "\n",
      "iteration 91 loss 0.4669247567653656 train acc/f1 0.78 / 0.78 test acc/f1 0.767 / 0.767 \n",
      "\n",
      "iteration 92 loss 0.46633180975914 train acc/f1 0.781 / 0.781 test acc/f1 0.767 / 0.767 \n",
      "\n",
      "iteration 93 loss 0.4657999575138092 train acc/f1 0.781 / 0.781 test acc/f1 0.767 / 0.767 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 94 loss 0.46528229117393494 train acc/f1 0.781 / 0.781 test acc/f1 0.766 / 0.766 \n",
      "\n",
      "iteration 95 loss 0.46478232741355896 train acc/f1 0.781 / 0.781 test acc/f1 0.77 / 0.77 \n",
      "\n",
      "iteration 96 loss 0.4642857015132904 train acc/f1 0.78 / 0.78 test acc/f1 0.765 / 0.765 \n",
      "\n",
      "iteration 97 loss 0.4637924134731293 train acc/f1 0.782 / 0.782 test acc/f1 0.774 / 0.774 \n",
      "\n",
      "iteration 98 loss 0.4632810950279236 train acc/f1 0.782 / 0.782 test acc/f1 0.766 / 0.766 \n",
      "\n",
      "iteration 99 loss 0.4627157747745514 train acc/f1 0.782 / 0.782 test acc/f1 0.774 / 0.774 \n",
      "\n",
      "iteration 100 loss 0.46203678846359253 train acc/f1 0.782 / 0.782 test acc/f1 0.769 / 0.769 \n",
      "\n",
      "iteration 101 loss 0.461482435464859 train acc/f1 0.782 / 0.782 test acc/f1 0.773 / 0.773 \n",
      "\n",
      "iteration 102 loss 0.46087971329689026 train acc/f1 0.782 / 0.782 test acc/f1 0.773 / 0.773 \n",
      "\n",
      "iteration 103 loss 0.46030527353286743 train acc/f1 0.783 / 0.783 test acc/f1 0.771 / 0.771 \n",
      "\n",
      "iteration 104 loss 0.45990249514579773 train acc/f1 0.783 / 0.783 test acc/f1 0.774 / 0.774 \n",
      "\n",
      "iteration 105 loss 0.4595189392566681 train acc/f1 0.783 / 0.782 test acc/f1 0.773 / 0.773 \n",
      "\n",
      "iteration 106 loss 0.4590887129306793 train acc/f1 0.784 / 0.784 test acc/f1 0.774 / 0.774 \n",
      "\n",
      "iteration 107 loss 0.45862191915512085 train acc/f1 0.784 / 0.784 test acc/f1 0.772 / 0.772 \n",
      "\n",
      "iteration 108 loss 0.45813867449760437 train acc/f1 0.786 / 0.785 test acc/f1 0.771 / 0.771 \n",
      "\n",
      "iteration 109 loss 0.45756852626800537 train acc/f1 0.785 / 0.785 test acc/f1 0.77 / 0.77 \n",
      "\n",
      "iteration 110 loss 0.4569800794124603 train acc/f1 0.785 / 0.785 test acc/f1 0.769 / 0.769 \n",
      "\n",
      "iteration 111 loss 0.4564761817455292 train acc/f1 0.785 / 0.785 test acc/f1 0.771 / 0.771 \n",
      "\n",
      "iteration 112 loss 0.456057071685791 train acc/f1 0.787 / 0.787 test acc/f1 0.77 / 0.77 \n",
      "\n",
      "iteration 113 loss 0.45566657185554504 train acc/f1 0.786 / 0.786 test acc/f1 0.771 / 0.771 \n",
      "\n",
      "iteration 114 loss 0.45527252554893494 train acc/f1 0.786 / 0.786 test acc/f1 0.771 / 0.771 \n",
      "\n",
      "iteration 115 loss 0.4548603296279907 train acc/f1 0.786 / 0.786 test acc/f1 0.771 / 0.771 \n",
      "\n",
      "iteration 116 loss 0.45441508293151855 train acc/f1 0.787 / 0.787 test acc/f1 0.772 / 0.772 \n",
      "\n",
      "iteration 117 loss 0.4539249539375305 train acc/f1 0.788 / 0.788 test acc/f1 0.77 / 0.77 \n",
      "\n",
      "iteration 118 loss 0.4533895254135132 train acc/f1 0.788 / 0.787 test acc/f1 0.772 / 0.772 \n",
      "\n",
      "iteration 119 loss 0.4528636634349823 train acc/f1 0.789 / 0.789 test acc/f1 0.772 / 0.772 \n",
      "\n",
      "iteration 120 loss 0.45237651467323303 train acc/f1 0.789 / 0.788 test acc/f1 0.771 / 0.771 \n",
      "\n",
      "iteration 121 loss 0.4519204795360565 train acc/f1 0.789 / 0.789 test acc/f1 0.774 / 0.774 \n",
      "\n",
      "iteration 122 loss 0.4514896273612976 train acc/f1 0.79 / 0.79 test acc/f1 0.776 / 0.776 \n",
      "\n",
      "iteration 123 loss 0.45108380913734436 train acc/f1 0.79 / 0.789 test acc/f1 0.776 / 0.776 \n",
      "\n",
      "iteration 124 loss 0.4507078230381012 train acc/f1 0.79 / 0.79 test acc/f1 0.775 / 0.775 \n",
      "\n",
      "iteration 125 loss 0.45037055015563965 train acc/f1 0.79 / 0.79 test acc/f1 0.774 / 0.774 \n",
      "\n",
      "iteration 126 loss 0.4500860571861267 train acc/f1 0.79 / 0.79 test acc/f1 0.774 / 0.774 \n",
      "\n",
      "iteration 127 loss 0.44983866810798645 train acc/f1 0.79 / 0.79 test acc/f1 0.773 / 0.773 \n",
      "\n",
      "iteration 128 loss 0.4495881497859955 train acc/f1 0.79 / 0.79 test acc/f1 0.775 / 0.775 \n",
      "\n",
      "iteration 129 loss 0.44915443658828735 train acc/f1 0.79 / 0.79 test acc/f1 0.775 / 0.775 \n",
      "\n",
      "iteration 130 loss 0.44851458072662354 train acc/f1 0.791 / 0.79 test acc/f1 0.778 / 0.778 \n",
      "\n",
      "iteration 131 loss 0.44782620668411255 train acc/f1 0.792 / 0.792 test acc/f1 0.782 / 0.782 \n",
      "\n",
      "iteration 132 loss 0.4473472535610199 train acc/f1 0.792 / 0.792 test acc/f1 0.78 / 0.78 \n",
      "\n",
      "iteration 133 loss 0.44710248708724976 train acc/f1 0.791 / 0.791 test acc/f1 0.778 / 0.778 \n",
      "\n",
      "iteration 134 loss 0.4469190537929535 train acc/f1 0.793 / 0.793 test acc/f1 0.778 / 0.778 \n",
      "\n",
      "iteration 135 loss 0.4466068744659424 train acc/f1 0.792 / 0.792 test acc/f1 0.78 / 0.78 \n",
      "\n",
      "iteration 136 loss 0.4460907578468323 train acc/f1 0.794 / 0.794 test acc/f1 0.779 / 0.779 \n",
      "\n",
      "iteration 137 loss 0.445520281791687 train acc/f1 0.793 / 0.793 test acc/f1 0.781 / 0.781 \n",
      "\n",
      "iteration 138 loss 0.4450693428516388 train acc/f1 0.794 / 0.794 test acc/f1 0.78 / 0.78 \n",
      "\n",
      "iteration 139 loss 0.4447736442089081 train acc/f1 0.794 / 0.794 test acc/f1 0.778 / 0.778 \n",
      "\n",
      "iteration 140 loss 0.4445309042930603 train acc/f1 0.794 / 0.793 test acc/f1 0.778 / 0.778 \n",
      "\n",
      "iteration 141 loss 0.4442112445831299 train acc/f1 0.794 / 0.794 test acc/f1 0.777 / 0.777 \n",
      "\n",
      "iteration 142 loss 0.4437882602214813 train acc/f1 0.794 / 0.794 test acc/f1 0.78 / 0.78 \n",
      "\n",
      "iteration 143 loss 0.4432944357395172 train acc/f1 0.794 / 0.794 test acc/f1 0.781 / 0.781 \n",
      "\n",
      "iteration 144 loss 0.442842572927475 train acc/f1 0.795 / 0.795 test acc/f1 0.781 / 0.781 \n",
      "\n",
      "iteration 145 loss 0.4424753785133362 train acc/f1 0.795 / 0.795 test acc/f1 0.781 / 0.781 \n",
      "\n",
      "iteration 146 loss 0.4421609342098236 train acc/f1 0.795 / 0.795 test acc/f1 0.776 / 0.776 \n",
      "\n",
      "iteration 147 loss 0.44189688563346863 train acc/f1 0.796 / 0.795 test acc/f1 0.776 / 0.776 \n",
      "\n",
      "iteration 148 loss 0.44161489605903625 train acc/f1 0.795 / 0.795 test acc/f1 0.777 / 0.777 \n",
      "\n",
      "iteration 149 loss 0.4412694573402405 train acc/f1 0.795 / 0.794 test acc/f1 0.778 / 0.778 \n",
      "\n",
      "iteration 150 loss 0.44087284803390503 train acc/f1 0.795 / 0.795 test acc/f1 0.775 / 0.775 \n",
      "\n",
      "iteration 151 loss 0.4404621720314026 train acc/f1 0.795 / 0.794 test acc/f1 0.778 / 0.778 \n",
      "\n",
      "iteration 152 loss 0.4400443732738495 train acc/f1 0.796 / 0.796 test acc/f1 0.776 / 0.776 \n",
      "\n",
      "iteration 153 loss 0.4396606981754303 train acc/f1 0.796 / 0.796 test acc/f1 0.778 / 0.778 \n",
      "\n",
      "iteration 154 loss 0.4393199384212494 train acc/f1 0.796 / 0.796 test acc/f1 0.779 / 0.779 \n",
      "\n",
      "iteration 155 loss 0.43899810314178467 train acc/f1 0.797 / 0.797 test acc/f1 0.775 / 0.775 \n",
      "\n",
      "iteration 156 loss 0.43869903683662415 train acc/f1 0.795 / 0.795 test acc/f1 0.778 / 0.778 \n",
      "\n",
      "iteration 157 loss 0.4384232461452484 train acc/f1 0.797 / 0.797 test acc/f1 0.775 / 0.775 \n",
      "\n",
      "iteration 158 loss 0.43816477060317993 train acc/f1 0.795 / 0.795 test acc/f1 0.772 / 0.772 \n",
      "\n",
      "iteration 159 loss 0.43793803453445435 train acc/f1 0.797 / 0.797 test acc/f1 0.772 / 0.772 \n",
      "\n",
      "iteration 160 loss 0.4377538561820984 train acc/f1 0.797 / 0.797 test acc/f1 0.769 / 0.769 \n",
      "\n",
      "iteration 161 loss 0.4375779330730438 train acc/f1 0.797 / 0.797 test acc/f1 0.772 / 0.772 \n",
      "\n",
      "iteration 162 loss 0.43736889958381653 train acc/f1 0.798 / 0.797 test acc/f1 0.77 / 0.77 \n",
      "\n",
      "iteration 163 loss 0.437013179063797 train acc/f1 0.798 / 0.798 test acc/f1 0.773 / 0.773 \n",
      "\n",
      "iteration 164 loss 0.4364936053752899 train acc/f1 0.798 / 0.798 test acc/f1 0.771 / 0.771 \n",
      "\n",
      "iteration 165 loss 0.4359007179737091 train acc/f1 0.798 / 0.798 test acc/f1 0.773 / 0.773 \n",
      "\n",
      "iteration 166 loss 0.4354253113269806 train acc/f1 0.798 / 0.798 test acc/f1 0.773 / 0.773 \n",
      "\n",
      "iteration 167 loss 0.435137003660202 train acc/f1 0.797 / 0.797 test acc/f1 0.77 / 0.77 \n",
      "\n",
      "iteration 168 loss 0.4349711239337921 train acc/f1 0.799 / 0.799 test acc/f1 0.77 / 0.77 \n",
      "\n",
      "iteration 169 loss 0.43481606245040894 train acc/f1 0.798 / 0.798 test acc/f1 0.771 / 0.771 \n",
      "\n",
      "iteration 170 loss 0.43456533551216125 train acc/f1 0.799 / 0.799 test acc/f1 0.77 / 0.77 \n",
      "\n",
      "iteration 171 loss 0.4341724216938019 train acc/f1 0.799 / 0.799 test acc/f1 0.771 / 0.771 \n",
      "\n",
      "iteration 172 loss 0.4336870014667511 train acc/f1 0.799 / 0.799 test acc/f1 0.772 / 0.772 \n",
      "\n",
      "iteration 173 loss 0.433217316865921 train acc/f1 0.799 / 0.799 test acc/f1 0.77 / 0.77 \n",
      "\n",
      "iteration 174 loss 0.43285515904426575 train acc/f1 0.799 / 0.799 test acc/f1 0.769 / 0.769 \n",
      "\n",
      "iteration 175 loss 0.4325828552246094 train acc/f1 0.8 / 0.8 test acc/f1 0.768 / 0.768 \n",
      "\n",
      "iteration 176 loss 0.4323628842830658 train acc/f1 0.8 / 0.8 test acc/f1 0.767 / 0.767 \n",
      "\n",
      "iteration 177 loss 0.43213602900505066 train acc/f1 0.8 / 0.8 test acc/f1 0.762 / 0.762 \n",
      "\n",
      "iteration 178 loss 0.4318678081035614 train acc/f1 0.8 / 0.8 test acc/f1 0.769 / 0.769 \n",
      "\n",
      "iteration 179 loss 0.4315298795700073 train acc/f1 0.801 / 0.801 test acc/f1 0.763 / 0.763 \n",
      "\n",
      "iteration 180 loss 0.43115314841270447 train acc/f1 0.801 / 0.801 test acc/f1 0.768 / 0.768 \n",
      "\n",
      "iteration 181 loss 0.4307454526424408 train acc/f1 0.801 / 0.801 test acc/f1 0.766 / 0.766 \n",
      "\n",
      "iteration 182 loss 0.4303583800792694 train acc/f1 0.802 / 0.801 test acc/f1 0.768 / 0.768 \n",
      "\n",
      "iteration 183 loss 0.43000203371047974 train acc/f1 0.801 / 0.801 test acc/f1 0.768 / 0.768 \n",
      "\n",
      "iteration 184 loss 0.4296860098838806 train acc/f1 0.801 / 0.801 test acc/f1 0.765 / 0.765 \n",
      "\n",
      "iteration 185 loss 0.42940056324005127 train acc/f1 0.802 / 0.802 test acc/f1 0.767 / 0.767 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 186 loss 0.4291337728500366 train acc/f1 0.803 / 0.802 test acc/f1 0.759 / 0.759 \n",
      "\n",
      "iteration 187 loss 0.42888396978378296 train acc/f1 0.802 / 0.802 test acc/f1 0.767 / 0.767 \n",
      "\n",
      "iteration 188 loss 0.42864927649497986 train acc/f1 0.803 / 0.803 test acc/f1 0.759 / 0.759 \n",
      "\n",
      "iteration 189 loss 0.42844974994659424 train acc/f1 0.803 / 0.803 test acc/f1 0.767 / 0.767 \n",
      "\n",
      "iteration 190 loss 0.42829838395118713 train acc/f1 0.802 / 0.802 test acc/f1 0.757 / 0.757 \n",
      "\n",
      "iteration 191 loss 0.4281703233718872 train acc/f1 0.803 / 0.803 test acc/f1 0.769 / 0.769 \n",
      "\n",
      "iteration 192 loss 0.42799344658851624 train acc/f1 0.803 / 0.803 test acc/f1 0.757 / 0.757 \n",
      "\n",
      "iteration 193 loss 0.42770856618881226 train acc/f1 0.803 / 0.803 test acc/f1 0.766 / 0.766 \n",
      "\n",
      "iteration 194 loss 0.42723360657691956 train acc/f1 0.803 / 0.803 test acc/f1 0.757 / 0.757 \n",
      "\n",
      "iteration 195 loss 0.4266447424888611 train acc/f1 0.804 / 0.804 test acc/f1 0.761 / 0.761 \n",
      "\n",
      "iteration 196 loss 0.42609256505966187 train acc/f1 0.803 / 0.803 test acc/f1 0.756 / 0.756 \n",
      "\n",
      "iteration 197 loss 0.42573896050453186 train acc/f1 0.804 / 0.804 test acc/f1 0.754 / 0.754 \n",
      "\n",
      "iteration 198 loss 0.42554664611816406 train acc/f1 0.805 / 0.805 test acc/f1 0.765 / 0.765 \n",
      "\n",
      "iteration 199 loss 0.4254259467124939 train acc/f1 0.803 / 0.803 test acc/f1 0.756 / 0.756 \n",
      "\n",
      "iteration 200 loss 0.42528894543647766 train acc/f1 0.805 / 0.805 test acc/f1 0.766 / 0.766 \n",
      "\n",
      "iteration 201 loss 0.42503613233566284 train acc/f1 0.804 / 0.804 test acc/f1 0.756 / 0.756 \n",
      "\n",
      "iteration 202 loss 0.4246474504470825 train acc/f1 0.806 / 0.805 test acc/f1 0.76 / 0.76 \n",
      "\n",
      "iteration 203 loss 0.4241598844528198 train acc/f1 0.804 / 0.804 test acc/f1 0.754 / 0.754 \n",
      "\n",
      "iteration 204 loss 0.4237074851989746 train acc/f1 0.806 / 0.806 test acc/f1 0.754 / 0.754 \n",
      "\n",
      "iteration 205 loss 0.4233498275279999 train acc/f1 0.805 / 0.805 test acc/f1 0.756 / 0.756 \n",
      "\n",
      "iteration 206 loss 0.42308273911476135 train acc/f1 0.805 / 0.805 test acc/f1 0.756 / 0.756 \n",
      "\n",
      "iteration 207 loss 0.4228653013706207 train acc/f1 0.807 / 0.806 test acc/f1 0.759 / 0.759 \n",
      "\n",
      "iteration 208 loss 0.42266982793807983 train acc/f1 0.805 / 0.805 test acc/f1 0.754 / 0.754 \n",
      "\n",
      "iteration 209 loss 0.42245590686798096 train acc/f1 0.806 / 0.806 test acc/f1 0.76 / 0.76 \n",
      "\n",
      "iteration 210 loss 0.4221888482570648 train acc/f1 0.805 / 0.805 test acc/f1 0.755 / 0.755 \n",
      "\n",
      "iteration 211 loss 0.42187365889549255 train acc/f1 0.806 / 0.805 test acc/f1 0.76 / 0.76 \n",
      "\n",
      "iteration 212 loss 0.42147088050842285 train acc/f1 0.805 / 0.805 test acc/f1 0.755 / 0.755 \n",
      "\n",
      "iteration 213 loss 0.4210863709449768 train acc/f1 0.807 / 0.806 test acc/f1 0.756 / 0.756 \n",
      "\n",
      "iteration 214 loss 0.42071443796157837 train acc/f1 0.806 / 0.806 test acc/f1 0.755 / 0.755 \n",
      "\n",
      "iteration 215 loss 0.42034491896629333 train acc/f1 0.808 / 0.808 test acc/f1 0.755 / 0.755 \n",
      "\n",
      "iteration 216 loss 0.41998112201690674 train acc/f1 0.807 / 0.807 test acc/f1 0.755 / 0.755 \n",
      "\n",
      "iteration 217 loss 0.4197031855583191 train acc/f1 0.808 / 0.808 test acc/f1 0.754 / 0.754 \n",
      "\n",
      "iteration 218 loss 0.4194006025791168 train acc/f1 0.808 / 0.808 test acc/f1 0.754 / 0.754 \n",
      "\n",
      "iteration 219 loss 0.4191315770149231 train acc/f1 0.808 / 0.808 test acc/f1 0.754 / 0.754 \n",
      "\n",
      "iteration 220 loss 0.4189019501209259 train acc/f1 0.809 / 0.808 test acc/f1 0.757 / 0.757 \n",
      "\n",
      "iteration 221 loss 0.41863691806793213 train acc/f1 0.807 / 0.807 test acc/f1 0.755 / 0.755 \n",
      "\n",
      "iteration 222 loss 0.41843873262405396 train acc/f1 0.807 / 0.807 test acc/f1 0.757 / 0.757 \n",
      "\n",
      "iteration 223 loss 0.41823479533195496 train acc/f1 0.808 / 0.808 test acc/f1 0.753 / 0.753 \n",
      "\n",
      "iteration 224 loss 0.4181106686592102 train acc/f1 0.808 / 0.808 test acc/f1 0.759 / 0.759 \n",
      "\n",
      "iteration 225 loss 0.41803133487701416 train acc/f1 0.808 / 0.808 test acc/f1 0.752 / 0.752 \n",
      "\n",
      "iteration 226 loss 0.4179996848106384 train acc/f1 0.809 / 0.808 test acc/f1 0.761 / 0.761 \n",
      "\n",
      "iteration 227 loss 0.41787785291671753 train acc/f1 0.807 / 0.807 test acc/f1 0.752 / 0.752 \n",
      "\n",
      "iteration 228 loss 0.41752082109451294 train acc/f1 0.809 / 0.809 test acc/f1 0.761 / 0.761 \n",
      "\n",
      "iteration 229 loss 0.41684988141059875 train acc/f1 0.809 / 0.809 test acc/f1 0.754 / 0.754 \n",
      "\n",
      "iteration 230 loss 0.41607752442359924 train acc/f1 0.81 / 0.809 test acc/f1 0.757 / 0.757 \n",
      "\n",
      "iteration 231 loss 0.4155353605747223 train acc/f1 0.81 / 0.81 test acc/f1 0.758 / 0.758 \n",
      "\n",
      "iteration 232 loss 0.415306955575943 train acc/f1 0.81 / 0.81 test acc/f1 0.755 / 0.755 \n",
      "\n",
      "iteration 233 loss 0.4152994453907013 train acc/f1 0.811 / 0.811 test acc/f1 0.759 / 0.759 \n",
      "\n",
      "iteration 234 loss 0.41523975133895874 train acc/f1 0.81 / 0.81 test acc/f1 0.754 / 0.754 \n",
      "\n",
      "iteration 235 loss 0.41497138142585754 train acc/f1 0.81 / 0.81 test acc/f1 0.759 / 0.759 \n",
      "\n",
      "iteration 236 loss 0.41445133090019226 train acc/f1 0.81 / 0.81 test acc/f1 0.755 / 0.755 \n",
      "\n",
      "iteration 237 loss 0.41389209032058716 train acc/f1 0.811 / 0.811 test acc/f1 0.757 / 0.757 \n",
      "\n",
      "iteration 238 loss 0.41347768902778625 train acc/f1 0.812 / 0.812 test acc/f1 0.76 / 0.76 \n",
      "\n",
      "iteration 239 loss 0.4132663607597351 train acc/f1 0.812 / 0.812 test acc/f1 0.754 / 0.754 \n",
      "\n",
      "iteration 240 loss 0.41314443945884705 train acc/f1 0.811 / 0.811 test acc/f1 0.76 / 0.76 \n",
      "\n",
      "iteration 241 loss 0.41297778487205505 train acc/f1 0.811 / 0.811 test acc/f1 0.756 / 0.756 \n",
      "\n",
      "iteration 242 loss 0.412680447101593 train acc/f1 0.813 / 0.813 test acc/f1 0.759 / 0.759 \n",
      "\n",
      "iteration 243 loss 0.41226035356521606 train acc/f1 0.811 / 0.811 test acc/f1 0.756 / 0.756 \n",
      "\n",
      "iteration 244 loss 0.4117906391620636 train acc/f1 0.813 / 0.813 test acc/f1 0.758 / 0.758 \n",
      "\n",
      "iteration 245 loss 0.4113699197769165 train acc/f1 0.814 / 0.814 test acc/f1 0.758 / 0.758 \n",
      "\n",
      "iteration 246 loss 0.4110349714756012 train acc/f1 0.814 / 0.814 test acc/f1 0.759 / 0.759 \n",
      "\n",
      "iteration 247 loss 0.4107871353626251 train acc/f1 0.813 / 0.813 test acc/f1 0.757 / 0.757 \n",
      "\n",
      "iteration 248 loss 0.41057413816452026 train acc/f1 0.815 / 0.815 test acc/f1 0.758 / 0.758 \n",
      "\n",
      "iteration 249 loss 0.41036832332611084 train acc/f1 0.813 / 0.813 test acc/f1 0.76 / 0.76 \n",
      "\n",
      "iteration 250 loss 0.41014519333839417 train acc/f1 0.814 / 0.814 test acc/f1 0.758 / 0.758 \n",
      "\n",
      "iteration 251 loss 0.40988320112228394 train acc/f1 0.813 / 0.813 test acc/f1 0.759 / 0.759 \n",
      "\n",
      "iteration 252 loss 0.409588098526001 train acc/f1 0.814 / 0.814 test acc/f1 0.758 / 0.758 \n",
      "\n",
      "iteration 253 loss 0.40925732254981995 train acc/f1 0.814 / 0.814 test acc/f1 0.759 / 0.759 \n",
      "\n",
      "iteration 254 loss 0.40890923142433167 train acc/f1 0.815 / 0.815 test acc/f1 0.757 / 0.757 \n",
      "\n",
      "iteration 255 loss 0.4085468351840973 train acc/f1 0.814 / 0.814 test acc/f1 0.762 / 0.762 \n",
      "\n",
      "iteration 256 loss 0.40819650888442993 train acc/f1 0.816 / 0.816 test acc/f1 0.759 / 0.759 \n",
      "\n",
      "iteration 257 loss 0.4078599214553833 train acc/f1 0.816 / 0.815 test acc/f1 0.762 / 0.762 \n",
      "\n",
      "iteration 258 loss 0.407540500164032 train acc/f1 0.817 / 0.816 test acc/f1 0.761 / 0.761 \n",
      "\n",
      "iteration 259 loss 0.40723928809165955 train acc/f1 0.817 / 0.816 test acc/f1 0.76 / 0.76 \n",
      "\n",
      "iteration 260 loss 0.40694648027420044 train acc/f1 0.816 / 0.816 test acc/f1 0.76 / 0.76 \n",
      "\n",
      "iteration 261 loss 0.40666815638542175 train acc/f1 0.816 / 0.816 test acc/f1 0.759 / 0.759 \n",
      "\n",
      "iteration 262 loss 0.4064009189605713 train acc/f1 0.816 / 0.816 test acc/f1 0.761 / 0.761 \n",
      "\n",
      "iteration 263 loss 0.40615931153297424 train acc/f1 0.816 / 0.816 test acc/f1 0.757 / 0.757 \n",
      "\n",
      "iteration 264 loss 0.405964195728302 train acc/f1 0.816 / 0.816 test acc/f1 0.757 / 0.757 \n",
      "\n",
      "iteration 265 loss 0.4058605134487152 train acc/f1 0.816 / 0.816 test acc/f1 0.756 / 0.756 \n",
      "\n",
      "iteration 266 loss 0.4059460759162903 train acc/f1 0.817 / 0.816 test acc/f1 0.764 / 0.764 \n",
      "\n",
      "iteration 267 loss 0.4062863290309906 train acc/f1 0.815 / 0.815 test acc/f1 0.756 / 0.756 \n",
      "\n",
      "iteration 268 loss 0.4069017171859741 train acc/f1 0.815 / 0.815 test acc/f1 0.758 / 0.758 \n",
      "\n",
      "iteration 269 loss 0.4071774482727051 train acc/f1 0.814 / 0.814 test acc/f1 0.756 / 0.756 \n",
      "\n",
      "iteration 270 loss 0.40647968649864197 train acc/f1 0.815 / 0.815 test acc/f1 0.764 / 0.764 \n",
      "\n",
      "iteration 271 loss 0.40473440289497375 train acc/f1 0.816 / 0.816 test acc/f1 0.761 / 0.761 \n",
      "\n",
      "iteration 272 loss 0.4035974442958832 train acc/f1 0.818 / 0.818 test acc/f1 0.759 / 0.759 \n",
      "\n",
      "iteration 273 loss 0.40384209156036377 train acc/f1 0.818 / 0.817 test acc/f1 0.761 / 0.761 \n",
      "\n",
      "iteration 274 loss 0.4044930934906006 train acc/f1 0.817 / 0.817 test acc/f1 0.759 / 0.759 \n",
      "\n",
      "iteration 275 loss 0.40424326062202454 train acc/f1 0.817 / 0.816 test acc/f1 0.761 / 0.761 \n",
      "\n",
      "iteration 276 loss 0.4030371606349945 train acc/f1 0.818 / 0.818 test acc/f1 0.763 / 0.763 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 277 loss 0.4022892117500305 train acc/f1 0.818 / 0.818 test acc/f1 0.758 / 0.758 \n",
      "\n",
      "iteration 278 loss 0.402522474527359 train acc/f1 0.818 / 0.818 test acc/f1 0.76 / 0.76 \n",
      "\n",
      "iteration 279 loss 0.4027116298675537 train acc/f1 0.818 / 0.818 test acc/f1 0.76 / 0.76 \n",
      "\n",
      "iteration 280 loss 0.4021422863006592 train acc/f1 0.818 / 0.818 test acc/f1 0.762 / 0.762 \n",
      "\n",
      "iteration 281 loss 0.4012068510055542 train acc/f1 0.819 / 0.819 test acc/f1 0.763 / 0.763 \n",
      "\n",
      "iteration 282 loss 0.40096279978752136 train acc/f1 0.819 / 0.819 test acc/f1 0.761 / 0.761 \n",
      "\n",
      "iteration 283 loss 0.4008772373199463 train acc/f1 0.82 / 0.82 test acc/f1 0.761 / 0.761 \n",
      "\n",
      "iteration 284 loss 0.40069910883903503 train acc/f1 0.819 / 0.819 test acc/f1 0.76 / 0.76 \n",
      "\n",
      "iteration 285 loss 0.40028896927833557 train acc/f1 0.82 / 0.82 test acc/f1 0.76 / 0.76 \n",
      "\n",
      "iteration 286 loss 0.39978405833244324 train acc/f1 0.82 / 0.82 test acc/f1 0.76 / 0.76 \n",
      "\n",
      "iteration 287 loss 0.39947938919067383 train acc/f1 0.82 / 0.819 test acc/f1 0.76 / 0.76 \n",
      "\n",
      "iteration 288 loss 0.3993358016014099 train acc/f1 0.82 / 0.82 test acc/f1 0.762 / 0.762 \n",
      "\n",
      "iteration 289 loss 0.39908427000045776 train acc/f1 0.821 / 0.821 test acc/f1 0.761 / 0.761 \n",
      "\n",
      "iteration 290 loss 0.3987945020198822 train acc/f1 0.82 / 0.82 test acc/f1 0.761 / 0.761 \n",
      "\n",
      "iteration 291 loss 0.3982357978820801 train acc/f1 0.82 / 0.82 test acc/f1 0.759 / 0.759 \n",
      "\n",
      "iteration 292 loss 0.39794281125068665 train acc/f1 0.82 / 0.82 test acc/f1 0.759 / 0.759 \n",
      "\n",
      "iteration 293 loss 0.39769262075424194 train acc/f1 0.82 / 0.82 test acc/f1 0.759 / 0.759 \n",
      "\n",
      "iteration 294 loss 0.39739862084388733 train acc/f1 0.82 / 0.82 test acc/f1 0.762 / 0.762 \n",
      "\n",
      "iteration 295 loss 0.3970930278301239 train acc/f1 0.821 / 0.821 test acc/f1 0.763 / 0.763 \n",
      "\n",
      "iteration 296 loss 0.39681747555732727 train acc/f1 0.821 / 0.821 test acc/f1 0.762 / 0.762 \n",
      "\n",
      "iteration 297 loss 0.39643189311027527 train acc/f1 0.822 / 0.822 test acc/f1 0.758 / 0.758 \n",
      "\n",
      "iteration 298 loss 0.39604052901268005 train acc/f1 0.823 / 0.823 test acc/f1 0.758 / 0.758 \n",
      "\n",
      "iteration 299 loss 0.3957395553588867 train acc/f1 0.823 / 0.823 test acc/f1 0.759 / 0.759 \n",
      "\n",
      "iteration 300 loss 0.39543652534484863 train acc/f1 0.822 / 0.822 test acc/f1 0.761 / 0.761 \n",
      "\n",
      "iteration 301 loss 0.39512211084365845 train acc/f1 0.824 / 0.823 test acc/f1 0.763 / 0.763 \n",
      "\n",
      "iteration 302 loss 0.3948543071746826 train acc/f1 0.823 / 0.823 test acc/f1 0.761 / 0.761 \n",
      "\n",
      "iteration 303 loss 0.39459243416786194 train acc/f1 0.824 / 0.824 test acc/f1 0.763 / 0.763 \n",
      "\n",
      "iteration 304 loss 0.3942960202693939 train acc/f1 0.824 / 0.824 test acc/f1 0.761 / 0.761 \n",
      "\n",
      "iteration 305 loss 0.39399224519729614 train acc/f1 0.824 / 0.824 test acc/f1 0.76 / 0.76 \n",
      "\n",
      "iteration 306 loss 0.39371559023857117 train acc/f1 0.824 / 0.823 test acc/f1 0.761 / 0.761 \n",
      "\n",
      "iteration 307 loss 0.39340323209762573 train acc/f1 0.824 / 0.824 test acc/f1 0.762 / 0.762 \n",
      "\n",
      "iteration 308 loss 0.3930823504924774 train acc/f1 0.825 / 0.825 test acc/f1 0.761 / 0.761 \n",
      "\n",
      "iteration 309 loss 0.392791211605072 train acc/f1 0.824 / 0.824 test acc/f1 0.762 / 0.762 \n",
      "\n",
      "iteration 310 loss 0.3924930691719055 train acc/f1 0.825 / 0.825 test acc/f1 0.759 / 0.759 \n",
      "\n",
      "iteration 311 loss 0.3922320008277893 train acc/f1 0.824 / 0.824 test acc/f1 0.764 / 0.764 \n",
      "\n",
      "iteration 312 loss 0.39200448989868164 train acc/f1 0.825 / 0.825 test acc/f1 0.76 / 0.76 \n",
      "\n",
      "iteration 313 loss 0.391807496547699 train acc/f1 0.825 / 0.825 test acc/f1 0.764 / 0.764 \n",
      "\n",
      "iteration 314 loss 0.39167487621307373 train acc/f1 0.825 / 0.824 test acc/f1 0.758 / 0.758 \n",
      "\n",
      "iteration 315 loss 0.39164865016937256 train acc/f1 0.825 / 0.825 test acc/f1 0.759 / 0.759 \n",
      "\n",
      "iteration 316 loss 0.39172378182411194 train acc/f1 0.825 / 0.825 test acc/f1 0.752 / 0.752 \n",
      "\n",
      "iteration 317 loss 0.39182960987091064 train acc/f1 0.824 / 0.824 test acc/f1 0.759 / 0.759 \n",
      "\n",
      "iteration 318 loss 0.3918563425540924 train acc/f1 0.824 / 0.824 test acc/f1 0.751 / 0.751 \n",
      "\n",
      "iteration 319 loss 0.3914352357387543 train acc/f1 0.824 / 0.824 test acc/f1 0.759 / 0.759 \n",
      "\n",
      "iteration 320 loss 0.39063867926597595 train acc/f1 0.825 / 0.825 test acc/f1 0.757 / 0.757 \n",
      "\n",
      "iteration 321 loss 0.3896236717700958 train acc/f1 0.826 / 0.826 test acc/f1 0.763 / 0.763 \n",
      "\n",
      "iteration 322 loss 0.3889361619949341 train acc/f1 0.827 / 0.827 test acc/f1 0.762 / 0.762 \n",
      "\n",
      "iteration 323 loss 0.3887663781642914 train acc/f1 0.827 / 0.827 test acc/f1 0.755 / 0.755 \n",
      "\n",
      "iteration 324 loss 0.388898104429245 train acc/f1 0.827 / 0.827 test acc/f1 0.759 / 0.759 \n",
      "\n",
      "iteration 325 loss 0.3890335261821747 train acc/f1 0.826 / 0.826 test acc/f1 0.754 / 0.754 \n",
      "\n",
      "iteration 326 loss 0.3888135552406311 train acc/f1 0.825 / 0.825 test acc/f1 0.763 / 0.763 \n",
      "\n",
      "iteration 327 loss 0.38826897740364075 train acc/f1 0.827 / 0.827 test acc/f1 0.762 / 0.762 \n",
      "\n",
      "iteration 328 loss 0.38758718967437744 train acc/f1 0.827 / 0.827 test acc/f1 0.763 / 0.763 \n",
      "\n",
      "iteration 329 loss 0.38709360361099243 train acc/f1 0.828 / 0.828 test acc/f1 0.764 / 0.764 \n",
      "\n",
      "iteration 330 loss 0.38688164949417114 train acc/f1 0.828 / 0.828 test acc/f1 0.761 / 0.761 \n",
      "\n",
      "iteration 331 loss 0.3868507742881775 train acc/f1 0.828 / 0.828 test acc/f1 0.762 / 0.762 \n",
      "\n",
      "iteration 332 loss 0.38682934641838074 train acc/f1 0.827 / 0.827 test acc/f1 0.758 / 0.758 \n",
      "\n",
      "iteration 333 loss 0.38665255904197693 train acc/f1 0.828 / 0.828 test acc/f1 0.762 / 0.762 \n",
      "\n",
      "iteration 334 loss 0.3863013982772827 train acc/f1 0.827 / 0.827 test acc/f1 0.759 / 0.759 \n",
      "\n",
      "iteration 335 loss 0.3858146369457245 train acc/f1 0.829 / 0.829 test acc/f1 0.764 / 0.764 \n",
      "\n",
      "iteration 336 loss 0.3853684663772583 train acc/f1 0.828 / 0.828 test acc/f1 0.764 / 0.764 \n",
      "\n",
      "iteration 337 loss 0.3850231170654297 train acc/f1 0.828 / 0.828 test acc/f1 0.764 / 0.764 \n",
      "\n",
      "iteration 338 loss 0.38480350375175476 train acc/f1 0.829 / 0.829 test acc/f1 0.765 / 0.765 \n",
      "\n",
      "iteration 339 loss 0.38465461134910583 train acc/f1 0.829 / 0.829 test acc/f1 0.762 / 0.762 \n",
      "\n",
      "iteration 340 loss 0.38452041149139404 train acc/f1 0.83 / 0.83 test acc/f1 0.764 / 0.764 \n",
      "\n",
      "iteration 341 loss 0.3843725323677063 train acc/f1 0.829 / 0.829 test acc/f1 0.762 / 0.762 \n",
      "\n",
      "iteration 342 loss 0.3841758072376251 train acc/f1 0.83 / 0.83 test acc/f1 0.763 / 0.763 \n",
      "\n",
      "iteration 343 loss 0.38392341136932373 train acc/f1 0.829 / 0.829 test acc/f1 0.763 / 0.763 \n",
      "\n",
      "iteration 344 loss 0.38359636068344116 train acc/f1 0.829 / 0.829 test acc/f1 0.763 / 0.763 \n",
      "\n",
      "iteration 345 loss 0.3832605481147766 train acc/f1 0.831 / 0.831 test acc/f1 0.764 / 0.764 \n",
      "\n",
      "iteration 346 loss 0.3829093873500824 train acc/f1 0.83 / 0.83 test acc/f1 0.764 / 0.764 \n",
      "\n",
      "iteration 347 loss 0.3825729787349701 train acc/f1 0.831 / 0.831 test acc/f1 0.764 / 0.764 \n",
      "\n",
      "iteration 348 loss 0.3822344243526459 train acc/f1 0.83 / 0.83 test acc/f1 0.765 / 0.765 \n",
      "\n",
      "iteration 349 loss 0.3819384276866913 train acc/f1 0.831 / 0.831 test acc/f1 0.764 / 0.764 \n",
      "\n",
      "iteration 350 loss 0.38164815306663513 train acc/f1 0.832 / 0.831 test acc/f1 0.764 / 0.764 \n",
      "\n",
      "iteration 351 loss 0.3813822865486145 train acc/f1 0.831 / 0.831 test acc/f1 0.764 / 0.764 \n",
      "\n",
      "iteration 352 loss 0.38111257553100586 train acc/f1 0.832 / 0.832 test acc/f1 0.762 / 0.762 \n",
      "\n",
      "iteration 353 loss 0.38086336851119995 train acc/f1 0.831 / 0.831 test acc/f1 0.76 / 0.76 \n",
      "\n",
      "iteration 354 loss 0.38061338663101196 train acc/f1 0.832 / 0.832 test acc/f1 0.761 / 0.761 \n",
      "\n",
      "iteration 355 loss 0.3803911507129669 train acc/f1 0.831 / 0.831 test acc/f1 0.761 / 0.761 \n",
      "\n",
      "iteration 356 loss 0.3801763951778412 train acc/f1 0.833 / 0.833 test acc/f1 0.761 / 0.761 \n",
      "\n",
      "iteration 357 loss 0.3799949586391449 train acc/f1 0.832 / 0.832 test acc/f1 0.76 / 0.76 \n",
      "\n",
      "iteration 358 loss 0.3798622786998749 train acc/f1 0.832 / 0.832 test acc/f1 0.757 / 0.757 \n",
      "\n",
      "iteration 359 loss 0.37983638048171997 train acc/f1 0.832 / 0.832 test acc/f1 0.762 / 0.762 \n",
      "\n",
      "iteration 360 loss 0.3800000548362732 train acc/f1 0.832 / 0.832 test acc/f1 0.756 / 0.756 \n",
      "\n",
      "iteration 361 loss 0.3804423213005066 train acc/f1 0.831 / 0.831 test acc/f1 0.757 / 0.757 \n",
      "\n",
      "iteration 362 loss 0.3812599182128906 train acc/f1 0.831 / 0.831 test acc/f1 0.757 / 0.757 \n",
      "\n",
      "iteration 363 loss 0.38208842277526855 train acc/f1 0.828 / 0.828 test acc/f1 0.757 / 0.757 \n",
      "\n",
      "iteration 364 loss 0.38214191794395447 train acc/f1 0.831 / 0.83 test acc/f1 0.754 / 0.754 \n",
      "\n",
      "iteration 365 loss 0.38042259216308594 train acc/f1 0.83 / 0.83 test acc/f1 0.762 / 0.762 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 366 loss 0.37826207280158997 train acc/f1 0.834 / 0.834 test acc/f1 0.759 / 0.759 \n",
      "\n",
      "iteration 367 loss 0.3774966299533844 train acc/f1 0.834 / 0.834 test acc/f1 0.759 / 0.759 \n",
      "\n",
      "iteration 368 loss 0.37823933362960815 train acc/f1 0.832 / 0.832 test acc/f1 0.758 / 0.758 \n",
      "\n",
      "iteration 369 loss 0.3790738880634308 train acc/f1 0.833 / 0.833 test acc/f1 0.755 / 0.755 \n",
      "\n",
      "iteration 370 loss 0.3785412907600403 train acc/f1 0.831 / 0.831 test acc/f1 0.761 / 0.761 \n",
      "\n",
      "iteration 371 loss 0.3770780563354492 train acc/f1 0.834 / 0.834 test acc/f1 0.758 / 0.758 \n",
      "\n",
      "iteration 372 loss 0.376318097114563 train acc/f1 0.835 / 0.835 test acc/f1 0.754 / 0.754 \n",
      "\n",
      "iteration 373 loss 0.3767380714416504 train acc/f1 0.834 / 0.834 test acc/f1 0.758 / 0.758 \n",
      "\n",
      "iteration 374 loss 0.37717995047569275 train acc/f1 0.834 / 0.833 test acc/f1 0.757 / 0.757 \n",
      "\n",
      "iteration 375 loss 0.37664422392845154 train acc/f1 0.833 / 0.833 test acc/f1 0.759 / 0.759 \n",
      "\n",
      "iteration 376 loss 0.37562236189842224 train acc/f1 0.835 / 0.835 test acc/f1 0.758 / 0.758 \n",
      "\n",
      "iteration 377 loss 0.37518516182899475 train acc/f1 0.837 / 0.836 test acc/f1 0.756 / 0.756 \n",
      "\n",
      "iteration 378 loss 0.3754556179046631 train acc/f1 0.835 / 0.835 test acc/f1 0.757 / 0.757 \n",
      "\n",
      "iteration 379 loss 0.37557703256607056 train acc/f1 0.834 / 0.834 test acc/f1 0.755 / 0.755 \n",
      "\n",
      "iteration 380 loss 0.3750481605529785 train acc/f1 0.835 / 0.835 test acc/f1 0.756 / 0.756 \n",
      "\n",
      "iteration 381 loss 0.3743523359298706 train acc/f1 0.836 / 0.836 test acc/f1 0.755 / 0.755 \n",
      "\n",
      "iteration 382 loss 0.3740957975387573 train acc/f1 0.837 / 0.836 test acc/f1 0.759 / 0.759 \n",
      "\n",
      "iteration 383 loss 0.3742174506187439 train acc/f1 0.836 / 0.836 test acc/f1 0.756 / 0.756 \n",
      "\n",
      "iteration 384 loss 0.3741791546344757 train acc/f1 0.836 / 0.836 test acc/f1 0.757 / 0.757 \n",
      "\n",
      "iteration 385 loss 0.37376099824905396 train acc/f1 0.836 / 0.836 test acc/f1 0.754 / 0.754 \n",
      "\n",
      "iteration 386 loss 0.37325021624565125 train acc/f1 0.837 / 0.837 test acc/f1 0.753 / 0.753 \n",
      "\n",
      "iteration 387 loss 0.3729848265647888 train acc/f1 0.837 / 0.837 test acc/f1 0.758 / 0.758 \n",
      "\n",
      "iteration 388 loss 0.3729695677757263 train acc/f1 0.838 / 0.838 test acc/f1 0.754 / 0.754 \n",
      "\n",
      "iteration 389 loss 0.37289923429489136 train acc/f1 0.836 / 0.836 test acc/f1 0.757 / 0.757 \n",
      "\n",
      "iteration 390 loss 0.37261173129081726 train acc/f1 0.838 / 0.838 test acc/f1 0.754 / 0.754 \n",
      "\n",
      "iteration 391 loss 0.372205525636673 train acc/f1 0.837 / 0.837 test acc/f1 0.755 / 0.755 \n",
      "\n",
      "iteration 392 loss 0.3718809187412262 train acc/f1 0.837 / 0.837 test acc/f1 0.756 / 0.756 \n",
      "\n",
      "iteration 393 loss 0.3717310428619385 train acc/f1 0.837 / 0.837 test acc/f1 0.754 / 0.754 \n",
      "\n",
      "iteration 394 loss 0.3716469705104828 train acc/f1 0.837 / 0.837 test acc/f1 0.756 / 0.756 \n",
      "\n",
      "iteration 395 loss 0.37149906158447266 train acc/f1 0.838 / 0.838 test acc/f1 0.754 / 0.754 \n",
      "\n",
      "iteration 396 loss 0.37123167514801025 train acc/f1 0.838 / 0.838 test acc/f1 0.755 / 0.755 \n",
      "\n",
      "iteration 397 loss 0.3709159195423126 train acc/f1 0.837 / 0.837 test acc/f1 0.752 / 0.752 \n",
      "\n",
      "iteration 398 loss 0.3706458508968353 train acc/f1 0.837 / 0.837 test acc/f1 0.752 / 0.752 \n",
      "\n",
      "iteration 399 loss 0.37045514583587646 train acc/f1 0.837 / 0.837 test acc/f1 0.754 / 0.754 \n",
      "\n",
      "iteration 400 loss 0.3703159987926483 train acc/f1 0.838 / 0.838 test acc/f1 0.752 / 0.752 \n",
      "\n",
      "iteration 401 loss 0.370165079832077 train acc/f1 0.838 / 0.838 test acc/f1 0.754 / 0.754 \n",
      "\n",
      "iteration 402 loss 0.36996713280677795 train acc/f1 0.838 / 0.838 test acc/f1 0.75 / 0.75 \n",
      "\n",
      "iteration 403 loss 0.36972546577453613 train acc/f1 0.838 / 0.838 test acc/f1 0.754 / 0.754 \n",
      "\n",
      "iteration 404 loss 0.36946699023246765 train acc/f1 0.837 / 0.837 test acc/f1 0.75 / 0.75 \n",
      "\n",
      "iteration 405 loss 0.3692198097705841 train acc/f1 0.838 / 0.838 test acc/f1 0.753 / 0.753 \n",
      "\n",
      "iteration 406 loss 0.36899998784065247 train acc/f1 0.838 / 0.838 test acc/f1 0.753 / 0.753 \n",
      "\n",
      "iteration 407 loss 0.3688046932220459 train acc/f1 0.838 / 0.838 test acc/f1 0.749 / 0.749 \n",
      "\n",
      "iteration 408 loss 0.368621289730072 train acc/f1 0.838 / 0.838 test acc/f1 0.752 / 0.752 \n",
      "\n",
      "iteration 409 loss 0.3684367537498474 train acc/f1 0.838 / 0.838 test acc/f1 0.75 / 0.75 \n",
      "\n",
      "iteration 410 loss 0.36824139952659607 train acc/f1 0.839 / 0.839 test acc/f1 0.752 / 0.752 \n",
      "\n",
      "iteration 411 loss 0.36803555488586426 train acc/f1 0.838 / 0.838 test acc/f1 0.75 / 0.75 \n",
      "\n",
      "iteration 412 loss 0.36781996488571167 train acc/f1 0.839 / 0.839 test acc/f1 0.753 / 0.753 \n",
      "\n",
      "iteration 413 loss 0.3675971031188965 train acc/f1 0.838 / 0.838 test acc/f1 0.75 / 0.75 \n",
      "\n",
      "iteration 414 loss 0.36737486720085144 train acc/f1 0.84 / 0.84 test acc/f1 0.753 / 0.753 \n",
      "\n",
      "iteration 415 loss 0.36715465784072876 train acc/f1 0.838 / 0.838 test acc/f1 0.75 / 0.75 \n",
      "\n",
      "iteration 416 loss 0.36693331599235535 train acc/f1 0.84 / 0.84 test acc/f1 0.752 / 0.752 \n",
      "\n",
      "iteration 417 loss 0.3667140305042267 train acc/f1 0.839 / 0.839 test acc/f1 0.749 / 0.749 \n",
      "\n",
      "iteration 418 loss 0.366498738527298 train acc/f1 0.84 / 0.84 test acc/f1 0.753 / 0.753 \n",
      "\n",
      "iteration 419 loss 0.3662993311882019 train acc/f1 0.839 / 0.839 test acc/f1 0.75 / 0.75 \n",
      "\n",
      "iteration 420 loss 0.3660895526409149 train acc/f1 0.84 / 0.84 test acc/f1 0.753 / 0.753 \n",
      "\n",
      "iteration 421 loss 0.36590272188186646 train acc/f1 0.839 / 0.839 test acc/f1 0.75 / 0.75 \n",
      "\n",
      "iteration 422 loss 0.36570748686790466 train acc/f1 0.841 / 0.841 test acc/f1 0.751 / 0.751 \n",
      "\n",
      "iteration 423 loss 0.36554545164108276 train acc/f1 0.839 / 0.839 test acc/f1 0.75 / 0.75 \n",
      "\n",
      "iteration 424 loss 0.36541372537612915 train acc/f1 0.841 / 0.841 test acc/f1 0.752 / 0.752 \n",
      "\n",
      "iteration 425 loss 0.3653303384780884 train acc/f1 0.839 / 0.839 test acc/f1 0.75 / 0.75 \n",
      "\n",
      "iteration 426 loss 0.36543968319892883 train acc/f1 0.841 / 0.841 test acc/f1 0.751 / 0.751 \n",
      "\n",
      "iteration 427 loss 0.3656652271747589 train acc/f1 0.839 / 0.839 test acc/f1 0.749 / 0.749 \n",
      "\n",
      "iteration 428 loss 0.36620867252349854 train acc/f1 0.84 / 0.84 test acc/f1 0.752 / 0.752 \n",
      "\n",
      "iteration 429 loss 0.3669043481349945 train acc/f1 0.837 / 0.837 test acc/f1 0.751 / 0.751 \n",
      "\n",
      "iteration 430 loss 0.3674667477607727 train acc/f1 0.838 / 0.838 test acc/f1 0.748 / 0.748 \n",
      "\n",
      "iteration 431 loss 0.3670637905597687 train acc/f1 0.837 / 0.837 test acc/f1 0.749 / 0.749 \n",
      "\n",
      "iteration 432 loss 0.3656626343727112 train acc/f1 0.84 / 0.84 test acc/f1 0.754 / 0.754 \n",
      "\n",
      "iteration 433 loss 0.3639133870601654 train acc/f1 0.839 / 0.839 test acc/f1 0.751 / 0.751 \n",
      "\n",
      "iteration 434 loss 0.36310920119285583 train acc/f1 0.842 / 0.842 test acc/f1 0.751 / 0.751 \n",
      "\n",
      "iteration 435 loss 0.36348989605903625 train acc/f1 0.842 / 0.842 test acc/f1 0.753 / 0.753 \n",
      "\n",
      "iteration 436 loss 0.36418578028678894 train acc/f1 0.839 / 0.839 test acc/f1 0.748 / 0.748 \n",
      "\n",
      "iteration 437 loss 0.3642841577529907 train acc/f1 0.84 / 0.84 test acc/f1 0.753 / 0.753 \n",
      "\n",
      "iteration 438 loss 0.36331358551979065 train acc/f1 0.839 / 0.839 test acc/f1 0.75 / 0.75 \n",
      "\n",
      "iteration 439 loss 0.3622608482837677 train acc/f1 0.843 / 0.842 test acc/f1 0.749 / 0.749 \n",
      "\n",
      "iteration 440 loss 0.36192211508750916 train acc/f1 0.843 / 0.843 test acc/f1 0.753 / 0.753 \n",
      "\n",
      "iteration 441 loss 0.3622567355632782 train acc/f1 0.84 / 0.84 test acc/f1 0.748 / 0.748 \n",
      "\n",
      "iteration 442 loss 0.36252081394195557 train acc/f1 0.841 / 0.841 test acc/f1 0.755 / 0.755 \n",
      "\n",
      "iteration 443 loss 0.3621463179588318 train acc/f1 0.84 / 0.84 test acc/f1 0.751 / 0.751 \n",
      "\n",
      "iteration 444 loss 0.3614055812358856 train acc/f1 0.842 / 0.842 test acc/f1 0.75 / 0.75 \n",
      "\n",
      "iteration 445 loss 0.36089250445365906 train acc/f1 0.842 / 0.842 test acc/f1 0.753 / 0.753 \n",
      "\n",
      "iteration 446 loss 0.36085575819015503 train acc/f1 0.841 / 0.841 test acc/f1 0.751 / 0.751 \n",
      "\n",
      "iteration 447 loss 0.3610093295574188 train acc/f1 0.842 / 0.841 test acc/f1 0.755 / 0.755 \n",
      "\n",
      "iteration 448 loss 0.3609195947647095 train acc/f1 0.841 / 0.841 test acc/f1 0.751 / 0.751 \n",
      "\n",
      "iteration 449 loss 0.3604885935783386 train acc/f1 0.842 / 0.842 test acc/f1 0.753 / 0.753 \n",
      "\n",
      "iteration 450 loss 0.35996684432029724 train acc/f1 0.843 / 0.843 test acc/f1 0.752 / 0.752 \n",
      "\n",
      "iteration 451 loss 0.3596850037574768 train acc/f1 0.842 / 0.842 test acc/f1 0.751 / 0.751 \n",
      "\n",
      "iteration 452 loss 0.3596649765968323 train acc/f1 0.842 / 0.842 test acc/f1 0.754 / 0.754 \n",
      "\n",
      "iteration 453 loss 0.3596591353416443 train acc/f1 0.841 / 0.841 test acc/f1 0.752 / 0.752 \n",
      "\n",
      "iteration 454 loss 0.35947850346565247 train acc/f1 0.842 / 0.842 test acc/f1 0.753 / 0.753 \n",
      "\n",
      "iteration 455 loss 0.3591184914112091 train acc/f1 0.842 / 0.842 test acc/f1 0.752 / 0.752 \n",
      "\n",
      "iteration 456 loss 0.35875412821769714 train acc/f1 0.842 / 0.842 test acc/f1 0.754 / 0.754 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 457 loss 0.35851672291755676 train acc/f1 0.843 / 0.843 test acc/f1 0.751 / 0.751 \n",
      "\n",
      "iteration 458 loss 0.35839998722076416 train acc/f1 0.843 / 0.843 test acc/f1 0.754 / 0.754 \n",
      "\n",
      "iteration 459 loss 0.3583206236362457 train acc/f1 0.843 / 0.842 test acc/f1 0.752 / 0.752 \n",
      "\n",
      "iteration 460 loss 0.3581770062446594 train acc/f1 0.843 / 0.843 test acc/f1 0.754 / 0.754 \n",
      "\n",
      "iteration 461 loss 0.35793837904930115 train acc/f1 0.843 / 0.843 test acc/f1 0.751 / 0.751 \n",
      "\n",
      "iteration 462 loss 0.357650488615036 train acc/f1 0.843 / 0.843 test acc/f1 0.755 / 0.755 \n",
      "\n",
      "iteration 463 loss 0.35738351941108704 train acc/f1 0.843 / 0.843 test acc/f1 0.755 / 0.755 \n",
      "\n",
      "iteration 464 loss 0.3571750223636627 train acc/f1 0.844 / 0.844 test acc/f1 0.751 / 0.751 \n",
      "\n",
      "iteration 465 loss 0.3570172190666199 train acc/f1 0.843 / 0.843 test acc/f1 0.753 / 0.753 \n",
      "\n",
      "iteration 466 loss 0.3568745255470276 train acc/f1 0.843 / 0.843 test acc/f1 0.749 / 0.749 \n",
      "\n",
      "iteration 467 loss 0.356717050075531 train acc/f1 0.843 / 0.843 test acc/f1 0.753 / 0.753 \n",
      "\n",
      "iteration 468 loss 0.3565288484096527 train acc/f1 0.844 / 0.844 test acc/f1 0.748 / 0.748 \n",
      "\n",
      "iteration 469 loss 0.3563057482242584 train acc/f1 0.844 / 0.844 test acc/f1 0.753 / 0.753 \n",
      "\n",
      "iteration 470 loss 0.35606810450553894 train acc/f1 0.844 / 0.844 test acc/f1 0.749 / 0.749 \n",
      "\n",
      "iteration 471 loss 0.35583195090293884 train acc/f1 0.844 / 0.844 test acc/f1 0.752 / 0.752 \n",
      "\n",
      "iteration 472 loss 0.3556113839149475 train acc/f1 0.844 / 0.844 test acc/f1 0.749 / 0.749 \n",
      "\n",
      "iteration 473 loss 0.3554065525531769 train acc/f1 0.844 / 0.844 test acc/f1 0.749 / 0.749 \n",
      "\n",
      "iteration 474 loss 0.3552156686782837 train acc/f1 0.845 / 0.845 test acc/f1 0.751 / 0.751 \n",
      "\n",
      "iteration 475 loss 0.3550388514995575 train acc/f1 0.844 / 0.844 test acc/f1 0.747 / 0.747 \n",
      "\n",
      "iteration 476 loss 0.3548664152622223 train acc/f1 0.845 / 0.845 test acc/f1 0.75 / 0.75 \n",
      "\n",
      "iteration 477 loss 0.3546944558620453 train acc/f1 0.844 / 0.844 test acc/f1 0.747 / 0.747 \n",
      "\n",
      "iteration 478 loss 0.35452407598495483 train acc/f1 0.845 / 0.845 test acc/f1 0.749 / 0.749 \n",
      "\n",
      "iteration 479 loss 0.35435667634010315 train acc/f1 0.845 / 0.844 test acc/f1 0.745 / 0.745 \n",
      "\n",
      "iteration 480 loss 0.35419097542762756 train acc/f1 0.845 / 0.845 test acc/f1 0.749 / 0.749 \n",
      "\n",
      "iteration 481 loss 0.3540336489677429 train acc/f1 0.844 / 0.844 test acc/f1 0.746 / 0.746 \n",
      "\n",
      "iteration 482 loss 0.3538871705532074 train acc/f1 0.844 / 0.844 test acc/f1 0.747 / 0.747 \n",
      "\n",
      "iteration 483 loss 0.35376638174057007 train acc/f1 0.845 / 0.844 test acc/f1 0.747 / 0.747 \n",
      "\n",
      "iteration 484 loss 0.35367849469184875 train acc/f1 0.844 / 0.844 test acc/f1 0.748 / 0.748 \n",
      "\n",
      "iteration 485 loss 0.35364794731140137 train acc/f1 0.845 / 0.845 test acc/f1 0.751 / 0.751 \n",
      "\n",
      "iteration 486 loss 0.35367828607559204 train acc/f1 0.844 / 0.844 test acc/f1 0.749 / 0.749 \n",
      "\n",
      "iteration 487 loss 0.3537960648536682 train acc/f1 0.844 / 0.844 test acc/f1 0.75 / 0.75 \n",
      "\n",
      "iteration 488 loss 0.3539506494998932 train acc/f1 0.845 / 0.845 test acc/f1 0.745 / 0.745 \n",
      "\n",
      "iteration 489 loss 0.3541255593299866 train acc/f1 0.844 / 0.844 test acc/f1 0.746 / 0.746 \n",
      "\n",
      "iteration 490 loss 0.35410526394844055 train acc/f1 0.844 / 0.844 test acc/f1 0.745 / 0.745 \n",
      "\n",
      "iteration 491 loss 0.35382744669914246 train acc/f1 0.845 / 0.845 test acc/f1 0.748 / 0.748 \n",
      "\n",
      "iteration 492 loss 0.35314252972602844 train acc/f1 0.845 / 0.845 test acc/f1 0.744 / 0.744 \n",
      "\n",
      "iteration 493 loss 0.35230863094329834 train acc/f1 0.845 / 0.844 test acc/f1 0.746 / 0.746 \n",
      "\n",
      "iteration 494 loss 0.35156577825546265 train acc/f1 0.846 / 0.846 test acc/f1 0.746 / 0.746 \n",
      "\n",
      "iteration 495 loss 0.35115957260131836 train acc/f1 0.847 / 0.847 test acc/f1 0.744 / 0.744 \n",
      "\n",
      "iteration 496 loss 0.35108673572540283 train acc/f1 0.846 / 0.846 test acc/f1 0.746 / 0.746 \n",
      "\n",
      "iteration 497 loss 0.35120701789855957 train acc/f1 0.846 / 0.846 test acc/f1 0.744 / 0.744 \n",
      "\n",
      "iteration 498 loss 0.3513277471065521 train acc/f1 0.845 / 0.845 test acc/f1 0.746 / 0.746 \n",
      "\n",
      "iteration 499 loss 0.35126993060112 train acc/f1 0.846 / 0.846 test acc/f1 0.744 / 0.744 \n",
      "\n",
      "iteration 500 loss 0.3510027527809143 train acc/f1 0.845 / 0.845 test acc/f1 0.747 / 0.747 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "net = Net(50)\n",
    "net.train(X_train, Y_train, X_test, Y_test, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = torch.nn.CrossEntropyLoss()\n",
    "# input = torch.randn(3, 5, requires_grad=True)\n",
    "# target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "# output = loss(input, target)\n",
    "# output\n",
    "# input, target.shape, Y_test.shape\n",
    "# torch.set_printoptions(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5267, 0.4733],\n",
       "        [0.7754, 0.2246],\n",
       "        [0.7114, 0.2886]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip = [\"dsfjghjbds\", \"the product was very nice\", test.iloc[8][\"Data\"]]\n",
    "temp = [ sentence_to_summed_vector(x) for x in ip]\n",
    "\n",
    "net.forward(torch.Tensor(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stuning even for the non gamer  this sound track was beautiful  it paints the senery in your mind so well i would recomend it even to people who hate vid  game music  i have played the game chrono cross but out of all of the games i have ever played it has the best music  it backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras  it would impress anyone who cares to listen  '"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[0][\"Data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[\"Data\"].apply(lambda x : sentence_to_sequence_vectors(x))\n",
    "X_test = test[\"Data\"].apply(lambda x : sentence_to_sequence_vectors(x))\n",
    "Y_train = torch.Tensor([[1, 0] if x == \"Positive\" else [0, 1] for x in train[\"Label\"]])\n",
    "Y_test = torch.Tensor([[1, 0] if x == \"Positive\" else [0, 1] for x in test[\"Label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "max_length = max([x.shape[0] for x in X_train]) + 47\n",
    "\n",
    "X_train = torch.Tensor(pad_sequences(X_train, maxlen=max_length, padding='pre'))\n",
    "X_test = torch.Tensor(pad_sequences(X_test, maxlen=max_length, padding='pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size = 2):\n",
    "        super().__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, batch_first = True)\n",
    "        self.output_layer = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr = 0.001)\n",
    "        pass\n",
    "    \n",
    "    def train(self, X_train, Y_train, X_test, Y_test, no_of_iterations, batch_size = 100):\n",
    "        \n",
    "        for itr in range(no_of_iterations):\n",
    "            itr_loss = 0\n",
    "            \n",
    "            before = time.time()\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                \n",
    "                X_batch = X_train[i:i+batch_size]\n",
    "                Y_batch = Y_train[i:i+batch_size]\n",
    "                \n",
    "                lstm_outputs, (h,c) = self.lstm(X_batch)\n",
    "                h = h.resize(h.shape[1], h.shape[2])\n",
    "                outputs = self.output_layer(h)\n",
    "                outputs = outputs.softmax(-1)\n",
    "                \n",
    "                batch_loss = - torch.sum(Y_batch * torch.log(outputs)) / len(X_batch) / max_length\n",
    "                itr_loss+= batch_loss\n",
    "                \n",
    "                batch_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                pass\n",
    "            \n",
    "            \n",
    "            predictions = self.forward(X_train)\n",
    "            train_acc = accuracy_score(y_pred = torch.argmax(predictions.detach(), -1), y_true = torch.argmax(Y_train, -1))\n",
    "            test_preds = self.forward(X_test)\n",
    "            test_acc = accuracy_score(y_pred = torch.argmax(test_preds.detach(), -1), y_true = torch.argmax(Y_test, -1))\n",
    "            \n",
    "            after = time.time()\n",
    "            print(\"iteration\", itr+1, \"loss\", round(itr_loss.item(), 6), \"train acc : \", round(train_acc,4), \"test acc : \", round(test_acc, 4), \"time taken : \", after-before)\n",
    "            pass\n",
    "        pass\n",
    "    \n",
    "    def forward(self, X):\n",
    "        lstm_outputs, (h, c) = self.lstm(X)\n",
    "        h = h.resize(h.size(1), h.size(2))\n",
    "        outputs = self.output_layer(h)\n",
    "        outputs = outputs.softmax(-1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/torch/tensor.py:339: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-bcc6e5919ec1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#rnet = RecurrentNet(50, 100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# loss, outputs = rnet.single_example(X_train[1], Y_train[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# loss, outputs.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-52822a4ee256>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, Y_train, X_test, Y_test, no_of_iterations, batch_size)\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mitr_loss\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "rnet = RecurrentNet(50, 100)\n",
    "rnet.train(X_train, Y_train, X_test, Y_test, 10)\n",
    "# loss, outputs = rnet.single_example(X_train[1], Y_train[1])\n",
    "# loss, outputs.shape\n",
    "# ops = rnet.forward(X_test)\n",
    "# ops.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([75, 100]), torch.Size([2, 75, 50]))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op, (h, c) = torch.nn.LSTM(20,50, batch_first = True, bidirectional=True)(torch.randn(75,10,20))\n",
    "op.shape, h.shape, c.shape\n",
    "#h.resize(h.size(1), h.size(2)).shape\n",
    "torch.cat((h[0], h[1]), -1).shape, h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentNetBidirectional(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size = 2):\n",
    "        super().__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, bidirectional = True, batch_first = True)\n",
    "        self.output_layer = torch.nn.Linear(2 * hidden_size, output_size)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr = 0.001)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        pass\n",
    "    \n",
    "    def train(self, X_train, Y_train, X_test, Y_test, no_of_iterations, batch_size = 100):\n",
    "        for itr in range(no_of_iterations):\n",
    "            itr_loss = 0\n",
    "            \n",
    "            before = time.time()\n",
    "            for i in range(0, X_train.shape[0], batch_size):\n",
    "                \n",
    "                X_batch = X_train[i:i+batch_size]\n",
    "                Y_batch = Y_train[i:i+batch_size]\n",
    "                \n",
    "                lstm_outputs, (h,c) = self.lstm(X_batch)\n",
    "                h = torch.cat((h[0],h[1]), -1)\n",
    "                outputs = self.output_layer(h)\n",
    "                outputs = outputs.softmax(-1)\n",
    "                \n",
    "                batch_loss = self.loss(input = outputs, target = Y_batch.argmax(-1))\n",
    "                itr_loss+= batch_loss\n",
    "                \n",
    "                batch_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                pass\n",
    "            \n",
    "            predictions = self.forward(X_train)\n",
    "            train_acc = accuracy_score(y_pred = torch.argmax(predictions.detach(), -1), y_true = torch.argmax(Y_train, -1))\n",
    "            test_preds = self.forward(X_test)\n",
    "            test_acc = accuracy_score(y_pred = torch.argmax(test_preds.detach(), -1), y_true = torch.argmax(Y_test, -1))\n",
    "            \n",
    "            after = time.time()\n",
    "            print(\"iteration\", itr+1, \"loss\", round(itr_loss.item(), 6), \"train acc : \", round(train_acc,4), \"test acc : \", round(test_acc, 4), \"time taken : \", after-before)\n",
    "            pass\n",
    "        pass\n",
    "    \n",
    "    def forward(self, X):\n",
    "        lstm_outputs, (h, c) = self.lstm(X)\n",
    "        h = torch.cat((h[0], h[1]), -1)\n",
    "        outputs = self.output_layer(h)\n",
    "        outputs = outputs.softmax(-1)\n",
    "        \n",
    "        return outputs\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 loss 62.562523 train acc :  0.517 test acc :  0.514 time taken :  34.75209999084473\n",
      "iteration 2 loss 62.301739 train acc :  0.5511 test acc :  0.553 time taken :  35.955974102020264\n",
      "iteration 3 loss 62.086433 train acc :  0.572 test acc :  0.571 time taken :  35.664610147476196\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-ee9413e141a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrbnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRecurrentNetBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrbnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_of_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-63-c9754ccd0b48>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, Y_train, X_test, Y_test, no_of_iterations, batch_size)\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mitr_loss\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "rbnet = RecurrentNetBidirectional(50, 10)\n",
    "rbnet.train(X_train, Y_train, X_test, Y_test, no_of_iterations = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 20, 20]) torch.Size([2, 100, 10]) torch.Size([2, 100, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 20, 20]), torch.Size([2, 100, 10]), torch.Size([2, 100, 10]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ops, (h,c) = torch.nn.LSTM(50,10, batch_first = True, bidirectional = True)(torch.randn(100, 20, 50))\n",
    "print(ops.shape, h.shape, c.shape)\n",
    "ops, (h,c) = torch.nn.LSTM(20, 10, batch_first = True, bidirectional = True)(ops)\n",
    "ops.shape, h.shape, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentNetBidirectionalMultiLayer(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size = 2):\n",
    "        super().__init__()\n",
    "        self.lstm1 = torch.nn.LSTM(input_size, hidden_size, bidirectional = True, batch_first = True)\n",
    "        self.lstm2 = torch.nn.LSTM(2 * hidden_size, hidden_size, bidirectional = True, batch_first = True)\n",
    "        self.output_layer = torch.nn.Linear(2 * hidden_size, output_size)\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr = 0.1, momentum = 0.9, weight_decay = 0.01, nesterov = True)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        pass\n",
    "    \n",
    "    def train(self, X_train, Y_train, X_test, Y_test, no_of_iterations, batch_size = 100):\n",
    "        for itr in range(no_of_iterations):\n",
    "            itr_loss = 0\n",
    "            \n",
    "            before = time.time()\n",
    "            for i in range(0, X_train.shape[0], batch_size):\n",
    "                X_batch = X_train[i:i+batch_size]\n",
    "                Y_batch = Y_train[i:i+batch_size]\n",
    "                \n",
    "                lstm1_outputs, (h,c) = self.lstm1(X_batch)\n",
    "                lstm2_outputs, (h,c) = self.lstm2(lstm1_outputs)\n",
    "                h = torch.cat((h[0],h[1]), -1)\n",
    "                outputs = self.output_layer(h)\n",
    "                outputs = outputs.softmax(-1)\n",
    "                \n",
    "                batch_loss = self.loss(input = outputs, target = Y_batch.argmax(-1))\n",
    "                \n",
    "                itr_loss+= batch_loss / (len(X_train)/batch_size)\n",
    "                \n",
    "                batch_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                pass\n",
    "            \n",
    "            predictions = self.forward(X_train)\n",
    "            train_acc = accuracy_score(y_pred = torch.argmax(predictions.detach(), -1), y_true = torch.argmax(Y_train, -1))\n",
    "            test_preds = self.forward(X_test)\n",
    "            test_acc = accuracy_score(y_pred = torch.argmax(test_preds.detach(), -1), y_true = torch.argmax(Y_test, -1))\n",
    "            \n",
    "            after = time.time()\n",
    "            print(\"iteration\", itr+1, \"loss\", round(itr_loss.item(), 6), \"train acc : \", round(train_acc,4), \"test acc : \", round(test_acc, 4), \"time taken : \", after-before)\n",
    "            pass\n",
    "        pass\n",
    "    \n",
    "    def forward(self, X):\n",
    "        lstm1_outputs, (h, c) = self.lstm1(X)\n",
    "        lstm2_outputs, (h, c) = self.lstm2(lstm1_outputs)\n",
    "        h = torch.cat((h[0], h[1]), -1)\n",
    "        outputs = self.output_layer(h)\n",
    "        outputs = outputs.softmax(-1)\n",
    "        \n",
    "        return outputs\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 loss 0.703355 train acc :  0.5066 test acc :  0.5 time taken :  305.50206685066223\n",
      "iteration 2 loss 0.70313 train acc :  0.5066 test acc :  0.5 time taken :  302.45164012908936\n",
      "iteration 3 loss 0.70313 train acc :  0.5066 test acc :  0.5 time taken :  292.7798430919647\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-fb9b5a98e890>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrbnetml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRecurrentNetBidirectionalMultiLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrbnetml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_of_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-112-3cf1c6e5b360>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, Y_train, X_test, Y_test, no_of_iterations, batch_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mitr_loss\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "rbnetml = RecurrentNetBidirectionalMultiLayer(50, 10)\n",
    "rbnetml.train(X_train, Y_train, X_test, Y_test, no_of_iterations = 10, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
