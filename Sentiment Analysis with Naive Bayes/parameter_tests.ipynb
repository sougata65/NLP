{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_path = os.getcwd()\n",
    "train = open(current_path + \"/Data/amazon_reviews_train.csv\",\"r+\")\n",
    "test = open(current_path + \"/Data/amazon_reviews_test.csv\",\"r+\")\n",
    "train = train.read()\n",
    "test = test.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn\n",
    "import string\n",
    "import textblob\n",
    "from sklearn.externals import joblib\n",
    "from nltk.stem.snowball import SnowballStemmer,PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    "from sklearn.svm import SVC, LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def messy_text_to_df(text):\n",
    "    documents = text.split(\"\\n\")\n",
    "    df = pd.DataFrame()\n",
    "    data = []\n",
    "    labels = []\n",
    "    for document in documents:\n",
    "        labels.append(document.split(\"\\t\",1)[0])\n",
    "        text = document.split('\\t')[1]\n",
    "        data.append(text)\n",
    "    labels = np.array(labels)\n",
    "    labels[np.where(labels=='__label__2')] = \"Positive\"\n",
    "    labels[np.where(labels=='__label__1')] = \"Negative\"\n",
    "    df[\"Data\"] = data\n",
    "    df[\"Label\"] = labels\n",
    "    \n",
    "    return df\n",
    "\n",
    "def remove_punctuation_and_numbers(text,replacements):\n",
    "    for key,value in replacements.items():\n",
    "        text = text.replace(key,value)\n",
    "    text = text.translate(str.maketrans('','',';\"#$%&\\'()*+/<=>?@[\\\\]^_`{|}~0123456789')).translate(str.maketrans('!.-:,','     '))\n",
    "    return text\n",
    "def remove_non_words(data,replacements):\n",
    "    res = data.apply(lambda x: remove_punctuation_and_numbers(x,replacements))\n",
    "    return res\n",
    "\n",
    "\n",
    "def remove_words_single(string,words_to_be_removed):\n",
    "    words = nltk.word_tokenize(string)\n",
    "    filtered_words = []\n",
    "    for i in range(len(words)):\n",
    "        if words[i] not in words_to_be_removed:\n",
    "            filtered_words.append(words[i])\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def remove_words(data,words_to_be_removed):\n",
    "    res = data.apply(lambda x : remove_words_single(x,words_to_be_removed))\n",
    "    return res\n",
    "    \n",
    "    for text,label in documents:\n",
    "        labels.append(document.split(\"\\t\",1)[0])\n",
    "        text = document.split('\\t')[1]\n",
    "        for key,value in replacements.items():\n",
    "            text = text.replace(key,value)\n",
    "            text = text.translate(str.maketrans('','',';\"#$%&\\'()*+/<=>?@[\\\\]^_`{|}~0123456789')).translate(str.maketrans('!.-:,','     '))\n",
    "        words = nltk.word_tokenize(text)\n",
    "        filtered_words = []\n",
    "        for i in range(len(words)):\n",
    "            if words[i] not in words_to_be_removed:\n",
    "                filtered_words.append(stemmer.stem(words[i]))\n",
    "        \n",
    "        res = ' '.join(filtered_words)\n",
    "        data.append(res)\n",
    "    labels = np.array(labels)\n",
    "    labels[np.where(labels=='__label__2')] = \"Positive\"\n",
    "    labels[np.where(labels=='__label__1')] = \"Negative\"\n",
    "    return data,labels\n",
    "\n",
    "def stem_single_string(string,nltkstemmer):\n",
    "    words = nltk.word_tokenize(string)\n",
    "    stemmed_list = []\n",
    "    for word in words:\n",
    "        stemmed_list.append(nltkstemmer.stem(word))\n",
    "    return ' '.join(stemmed_list)\n",
    "    \n",
    "\n",
    "def stem(data):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    res = data.apply(lambda x : stem_single_string(x,stemmer))\n",
    "    return res\n",
    "\n",
    "def find_rare_words(data,max_frequency=4):\n",
    "    \n",
    "    vectoriser = get_vectorizer(data)\n",
    "    \n",
    "    \n",
    "    temp = ' '.join(data)\n",
    "    frequencies = (nltk.FreqDist(nltk.word_tokenize(temp)))\n",
    "    \n",
    "    fs = np.array(frequencies.most_common())\n",
    "    fs = pd.DataFrame(fs)\n",
    "    fs.columns = [\"word\",\"count\"]\n",
    "    fs[\"freq\"] = fs[\"count\"].astype(int)\n",
    "    fs = fs.drop(\"count\",axis=1)\n",
    "    \n",
    "    rare_words = list(fs[fs[\"freq\"]<=max_frequency][\"word\"])\n",
    "    \n",
    "    return rare_words\n",
    "\n",
    "def get_vectorizer(data,vectorizer=\"CountVectorizer\"):\n",
    "    \n",
    "    if vectorizer == \"Tfidf\":\n",
    "        tfidf = TfidfVectorizer()\n",
    "        tfidf.fit(data)\n",
    "        return tfidf\n",
    "    cv = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "    cv.fit(data)\n",
    "    \n",
    "    return cv\n",
    "\n",
    "def vectorize_data(data,vectorizer=\"CountVectorizer\"):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "    \n",
    "    if vectorizer == \"Tfidf\":\n",
    "        tfidf = TfidfVectorizer()\n",
    "        tfidf.fit(data)\n",
    "        \n",
    "        return tfidf.transform(data).toarray()\n",
    "    cv = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "    cv.fit(data)\n",
    "    \n",
    "    return cv.transform(data).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols_stopwords_and_stem(data):\n",
    "    data = messy_text_to_df(data)\n",
    "    data[\"Data\"] = remove_non_words(data[\"Data\"],replacements)\n",
    "    data[\"Data\"] = remove_words(data[\"Data\"],stopwords)\n",
    "    data[\"Data\"] = stem(data[\"Data\"])\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "replacements = {\"can't\" : 'can not',\"shan't\":'shall not',\"won't\":'will not',\"'ve\" : \" have\", \"'d\" : \" would\", \"'m\" : \" am\", \"'ll\" : \" will\", \"'s\" : \"\", \"n't\" : \" not\",\"'re\" : \"are\"}\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.remove(\"not\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stopwords = set([\"can\",\"could\",\"would\",\"have\",\"go\",\"went\",\"zero\",\"one\",\"two\",\"three\",\"four\",\"five\",\"six\",\"seven\",\"eight\",\"nine\",\"ten\"]) | set(stopwords)\n",
    "\n",
    "train = remove_symbols_stopwords_and_stem(train)\n",
    "test = remove_symbols_stopwords_and_stem(test)\n",
    "\n",
    "rare_words = find_rare_words(train[\"Data\"])\n",
    "\n",
    "train[\"Data\"] = remove_words(train[\"Data\"],rare_words)\n",
    "test[\"Data\"] = remove_words(test[\"Data\"],rare_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stune even non this sound track beauti it pain...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the best soundtrack ever anyth i read lot revi...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amaz this soundtrack favorit music time hand t...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>excel soundtrack i truli like soundtrack i enj...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rememb pull your jaw off the floor after hear ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Data     Label\n",
       "0  stune even non this sound track beauti it pain...  Positive\n",
       "1  the best soundtrack ever anyth i read lot revi...  Positive\n",
       "2  amaz this soundtrack favorit music time hand t...  Positive\n",
       "3  excel soundtrack i truli like soundtrack i enj...  Positive\n",
       "4  rememb pull your jaw off the floor after hear ...  Positive"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = get_vectorizer(train[\"Data\"], vectorizer = \"CountVectorizer\")\n",
    "tfidf_vectorizer = get_vectorizer(train[\"Data\"], vectorizer = \"Tfidf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing to readme.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_row_to_readme(list_of_values, file=current_path+'/readme.md'):\n",
    "    vals = [ str(x) for x in list_of_values ]\n",
    "    out_line = '|' + '|'.join(vals) + '|\\n'\n",
    "    writer = open(file, \"a+\")\n",
    "    writer.write(out_line)\n",
    "    pass\n",
    "\n",
    "# insert_row_to_readme(['Model', 'Vectorizer', 'Accuracy', 'F1(Macro)', 'Hyperparameters'])\n",
    "# insert_row_to_readme(['---','---','---','---','---'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score = sklearn.metrics.f1_score\n",
    "accuracy_score = sklearn.metrics.accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vectorizer =  CountVectorizer \n",
      "Model =  MultinomialNB \n",
      "Accuracy :  0.821 \n",
      "F1 Scores :  0.820999820999821\n",
      "\n",
      "Vectorizer =  TfidfVectorizer \n",
      "Model =  MultinomialNB \n",
      "Accuracy :  0.818 \n",
      "F1 Scores :  0.8179883512544802\n",
      "\n",
      "Vectorizer =  CountVectorizer \n",
      "Model =  GaussianNB \n",
      "Accuracy :  0.692 \n",
      "F1 Scores :  0.6917225502952657\n",
      "\n",
      "Vectorizer =  TfidfVectorizer \n",
      "Model =  GaussianNB \n",
      "Accuracy :  0.695 \n",
      "F1 Scores :  0.694706613055146\n"
     ]
    }
   ],
   "source": [
    "for nb_classifier in [MultinomialNB, GaussianNB]:\n",
    "    for vectorizer in [count_vectorizer, tfidf_vectorizer]:        \n",
    "        model = nb_classifier()\n",
    "        model.fit(vectorizer.transform(train[\"Data\"]).toarray(), train[\"Label\"])\n",
    "        preds = model.predict(vectorizer.transform(test[\"Data\"]).toarray())\n",
    "        f1_scores = f1_score(y_true = test[\"Label\"], y_pred = preds, average = 'macro')\n",
    "        accuracy = accuracy_score(y_true = test[\"Label\"], y_pred = preds)\n",
    "        list_of_values = [model.__class__.__name__, vectorizer.__class__.__name__, accuracy, f1_scores, None]\n",
    "        insert_row_to_readme(list_of_values)\n",
    "        print(\"\\nVectorizer = \", vectorizer.__class__.__name__, \"\\nModel = \", model.__class__.__name__, \"\\nAccuracy : \",accuracy,\"\\nF1 Scores : \", f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "count_scores = []\n",
    "tfidf_scores = []\n",
    "for vectorizer in [count_vectorizer,tfidf_vectorizer]:\n",
    "    \n",
    "    for c in range(-10,11):\n",
    "        model = LinearSVC(C=10**c, max_iter=10000)\n",
    "        model.fit(vectorizer.transform(train[\"Data\"]).toarray(), train[\"Label\"])\n",
    "        preds = model.predict(vectorizer.transform(test[\"Data\"]).toarray())\n",
    "        f1_scores = f1_score(y_true = test[\"Label\"], y_pred = preds, average = 'macro')\n",
    "        accuracy = accuracy_score(y_true = test[\"Label\"], y_pred = preds)\n",
    "        \n",
    "        \n",
    "        list_of_values = [model.__class__.__name__, vectorizer.__class__.__name__, accuracy, f1_scores, {'C' : 10**c}]\n",
    "        insert_row_to_readme(list_of_values)\n",
    "#         if vectorizer.__class__.__name__ == \"CountVectorizer\":\n",
    "#             count_scores.append((\"C = \"+str(c), accuracy,f1_scores))\n",
    "#             pass\n",
    "#         if vectorizer.__class__.__name__ == \"TfidfVectorizer\":\n",
    "#             tfidf_scores.append((\"C = \"+str(c), accuracy,f1_scores))\n",
    "#             pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# count_scores = []\n",
    "# tfidf_scores = []\n",
    "# model = SVC()\n",
    "# for vectorizer in [count_vectorizer,tfidf_vectorizer]:\n",
    "#     for c in range(-10,11):\n",
    "#         model.C = 10**c\n",
    "#         for kernel in ['linear', 'rbf']:\n",
    "#             model.kernel = kernel\n",
    "#             if kernel == 'linear':\n",
    "#                 model.fit(vectorizer.transform(train[\"Data\"]).toarray(), train[\"Label\"])\n",
    "#                 preds = model.predict(vectorizer.transform(test[\"Data\"]).toarray())\n",
    "#                 f1_scores = f1_score(y_true = test[\"Label\"], y_pred = preds, average = 'macro')\n",
    "#                 accuracy = accuracy_score(y_true = test[\"Label\"], y_pred = preds)\n",
    "#                 list_of_values = [model.__class__.__name__, vectorizer.__class__.__name__, accuracy, f1_scores, {'C' : 10**c, 'kernel' : kernel}]\n",
    "#                 insert_row_to_readme(list_of_values)\n",
    "#                 pass\n",
    "#             if kernel == \"rbf\":\n",
    "#                 for gamma in range(-10, 11):\n",
    "#                     model.gamma = 10**gamma\n",
    "#                     model.fit(vectorizer.transform(train[\"Data\"]).toarray(), train[\"Label\"])\n",
    "#                     preds = model.predict(vectorizer.transform(test[\"Data\"]).toarray())\n",
    "#                     f1_scores = f1_score(y_true = test[\"Label\"], y_pred = preds, average = 'macro')\n",
    "#                     accuracy = accuracy_score(y_true = test[\"Label\"], y_pred = preds)\n",
    "#                     list_of_values = [model.__class__.__name__, vectorizer.__class__.__name__, accuracy, f1_scores, {'C' : 10**c, 'kernel' : kernel, \"gamma\" : 10**gamma}]\n",
    "#                     insert_row_to_readme(list_of_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C :  0.1 gamma :  0.0001 acc :  0.5 f1 :  0.3333333333333333\n",
      "Time taken :  675.2670350074768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C :  0.1 gamma :  0.001 acc :  0.5 f1 :  0.3333333333333333\n",
      "Time taken :  693.1976277828217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C :  0.1 gamma :  0.01 acc :  0.5 f1 :  0.3333333333333333\n",
      "Time taken :  694.112625837326\n",
      "C :  0.1 gamma :  0.1 acc :  0.765 f1 :  0.7593670226328115\n",
      "Time taken :  716.3489379882812\n",
      "C :  0.1 gamma :  1 acc :  0.829 f1 :  0.828979306496086\n",
      "Time taken :  663.3432328701019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C :  0.1 gamma :  10 acc :  0.5 f1 :  0.3333333333333333\n",
      "Time taken :  971.3392922878265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C :  0.1 gamma :  100 acc :  0.5 f1 :  0.3333333333333333\n",
      "Time taken :  1005.3757529258728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C :  1 gamma :  0.0001 acc :  0.5 f1 :  0.3333333333333333\n",
      "Time taken :  677.2333979606628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C :  1 gamma :  0.001 acc :  0.5 f1 :  0.3333333333333333\n",
      "Time taken :  675.7112131118774\n",
      "C :  1 gamma :  0.01 acc :  0.798 f1 :  0.7974523110490768\n",
      "Time taken :  630.7418649196625\n",
      "C :  1 gamma :  0.1 acc :  0.861 f1 :  0.8609931886662446\n",
      "Time taken :  449.7343330383301\n",
      "C :  1 gamma :  1 acc :  0.876 f1 :  0.875987598759876\n",
      "Time taken :  469.9936339855194\n",
      "C :  1 gamma :  10 acc :  0.538 f1 :  0.4126292342821109\n",
      "Time taken :  1180.2235629558563\n",
      "C :  1 gamma :  100 acc :  0.538 f1 :  0.4126292342821109\n",
      "Time taken :  1053.8554270267487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C :  10 gamma :  0.0001 acc :  0.5 f1 :  0.3333333333333333\n",
      "Time taken :  638.2842588424683\n",
      "C :  10 gamma :  0.001 acc :  0.799 f1 :  0.7984758356485218\n",
      "Time taken :  630.3827059268951\n",
      "C :  10 gamma :  0.01 acc :  0.864 f1 :  0.8639804131794978\n",
      "Time taken :  433.34253096580505\n",
      "C :  10 gamma :  0.1 acc :  0.873 f1 :  0.8729785333721398\n",
      "Time taken :  312.2916078567505\n",
      "C :  10 gamma :  1 acc :  0.877 f1 :  0.8769851151989391\n",
      "Time taken :  911.1613948345184\n",
      "C :  10 gamma :  10 acc :  0.538 f1 :  0.4126292342821109\n",
      "Time taken :  2256.328818321228\n",
      "C :  10 gamma :  100 acc :  0.538 f1 :  0.4126292342821109\n",
      "Time taken :  2156.4311220645905\n",
      "C :  100 gamma :  0.0001 acc :  0.799 f1 :  0.7984758356485218\n",
      "Time taken :  628.8431558609009\n",
      "C :  100 gamma :  0.001 acc :  0.864 f1 :  0.8639804131794978\n",
      "Time taken :  429.485271692276\n",
      "C :  100 gamma :  0.01 acc :  0.871 f1 :  0.8709781953150082\n",
      "Time taken :  296.7538843154907\n",
      "C :  100 gamma :  0.1 acc :  0.849 f1 :  0.8489817267889415\n",
      "Time taken :  292.68509006500244\n",
      "C :  100 gamma :  1 acc :  0.877 f1 :  0.8769851151989391\n",
      "Time taken :  911.035829782486\n",
      "C :  100 gamma :  10 acc :  0.538 f1 :  0.4126292342821109\n",
      "Time taken :  2265.5358221530914\n",
      "C :  100 gamma :  100 acc :  0.538 f1 :  0.4126292342821109\n",
      "Time taken :  2156.200419187546\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for c in [0.1, 1, 10, 100]:\n",
    "    for gamma in [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "        \n",
    "        before = time.time()\n",
    "        svc = SVC(C=c, kernel = 'rbf', gamma = gamma)\n",
    "        svc.fit(tfidf_vectorizer.transform(train[\"Data\"]).toarray(), train[\"Label\"])\n",
    "        preds = svc.predict(tfidf_vectorizer.transform(test[\"Data\"]).toarray())\n",
    "        f1 = sklearn.metrics.f1_score(y_true = test[\"Label\"], y_pred = preds, average = \"macro\")\n",
    "        acc = sklearn.metrics.accuracy_score(y_true = test[\"Label\"], y_pred = preds)\n",
    "        print('C : ', c, 'gamma : ', gamma, 'acc : ', acc, 'f1 : ', f1)\n",
    "        after = time.time()\n",
    "        insert_row_to_readme([\"SVC\", \"Tfidf\", acc, f1, {\"C\" : c, \"kernel\" : \"rbf\", \"gamma\" : gamma}])\n",
    "        print(\"Time taken : \", after - before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.876"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.f1_score(y_true = test[\"Label\"], y_pred = preds, average = \"macro\")\n",
    "sklearn.metrics.accuracy_score(y_true = test[\"Label\"], y_pred = preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#joblib.dump(model,\"sentiment_analysis_naive_bayes_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_Score = sklearn.metrics.f1_score(y_pred = preds,y_true = test[\"Label\"], average = None)\n",
    "Accuracy = sklearn.metrics.accuracy_score(y_true = test[\"Label\"],y_pred = preds)\n",
    "print(\"Model trained.\\n\"\"F1 Score : \",F1_Score,\"\\nAccuracy : \",Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
