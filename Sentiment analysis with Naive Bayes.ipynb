{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = open(\"/Users/sougata-8718/Downloads/amazon_reviews_train.csv\",\"r+\")\n",
    "test = open(\"/Users/sougata-8718/Downloads/amazon_reviews_test.csv\",\"r+\")\n",
    "train = train.read()\n",
    "test = test.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.core._multiarray_umath'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.core._multiarray_umath'"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.core._multiarray_umath'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.core._multiarray_umath'"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.core._multiarray_umath'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.core._multiarray_umath'"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.core._multiarray_umath'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.core._multiarray_umath'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn\n",
    "import string\n",
    "import textblob\n",
    "from sklearn.externals import joblib\n",
    "from nltk.stem.snowball import SnowballStemmer,PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def messy_text_to_df(text):\n",
    "    documents = text.split(\"\\n\")\n",
    "    df = pd.DataFrame()\n",
    "    data = []\n",
    "    labels = []\n",
    "    for document in documents:\n",
    "        labels.append(document.split(\"\\t\",1)[0])\n",
    "        text = document.split('\\t')[1]\n",
    "        data.append(text)\n",
    "    labels = np.array(labels)\n",
    "    labels[np.where(labels=='__label__2')] = \"Positive\"\n",
    "    labels[np.where(labels=='__label__1')] = \"Negative\"\n",
    "    df[\"Data\"] = data\n",
    "    df[\"Label\"] = labels\n",
    "    \n",
    "    return df\n",
    "\n",
    "def remove_punctuation_and_numbers(text,replacements):\n",
    "    for key,value in replacements.items():\n",
    "        text = text.replace(key,value)\n",
    "    text = text.translate(str.maketrans('','',';\"#$%&\\'()*+/<=>?@[\\\\]^_`{|}~0123456789')).translate(str.maketrans('!.-:,','     '))\n",
    "    return text\n",
    "def remove_non_words(data,replacements):\n",
    "    res = data.apply(lambda x: remove_punctuation_and_numbers(x,replacements))\n",
    "    return res\n",
    "\n",
    "\n",
    "def remove_words_single(string,words_to_be_removed):\n",
    "    words = nltk.word_tokenize(string)\n",
    "    filtered_words = []\n",
    "    for i in range(len(words)):\n",
    "        if words[i] not in words_to_be_removed:\n",
    "            filtered_words.append(words[i])\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def remove_words(data,words_to_be_removed):\n",
    "    res = data.apply(lambda x : remove_words_single(x,words_to_be_removed))\n",
    "    return res\n",
    "    \n",
    "    for text,label in documents:\n",
    "        labels.append(document.split(\"\\t\",1)[0])\n",
    "        text = document.split('\\t')[1]\n",
    "        for key,value in replacements.items():\n",
    "            text = text.replace(key,value)\n",
    "            text = text.translate(str.maketrans('','',';\"#$%&\\'()*+/<=>?@[\\\\]^_`{|}~0123456789')).translate(str.maketrans('!.-:,','     '))\n",
    "        words = nltk.word_tokenize(text)\n",
    "        filtered_words = []\n",
    "        for i in range(len(words)):\n",
    "            if words[i] not in words_to_be_removed:\n",
    "                filtered_words.append(stemmer.stem(words[i]))\n",
    "        \n",
    "        res = ' '.join(filtered_words)\n",
    "        data.append(res)\n",
    "    labels = np.array(labels)\n",
    "    labels[np.where(labels=='__label__2')] = \"Positive\"\n",
    "    labels[np.where(labels=='__label__1')] = \"Negative\"\n",
    "    return data,labels\n",
    "\n",
    "def stem_single_string(string,nltkstemmer):\n",
    "    words = nltk.word_tokenize(string)\n",
    "    stemmed_list = []\n",
    "    for word in words:\n",
    "        stemmed_list.append(nltkstemmer.stem(word))\n",
    "    return ' '.join(stemmed_list)\n",
    "    \n",
    "\n",
    "def stem(data):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    res = data.apply(lambda x : stem_single_string(x,stemmer))\n",
    "    return res\n",
    "\n",
    "def find_rare_words(data,max_frequency=4):\n",
    "    \n",
    "    vectoriser = get_vectorizer(data)\n",
    "    \n",
    "    \n",
    "    temp = ' '.join(data)\n",
    "    frequencies = (nltk.FreqDist(nltk.word_tokenize(temp)))\n",
    "    \n",
    "    fs = np.array(frequencies.most_common())\n",
    "    fs = pd.DataFrame(fs)\n",
    "    fs.columns = [\"word\",\"count\"]\n",
    "    fs[\"freq\"] = fs[\"count\"].astype(int)\n",
    "    fs = fs.drop(\"count\",axis=1)\n",
    "    \n",
    "    rare_words = list(fs[fs[\"freq\"]<=max_frequency][\"word\"])\n",
    "    \n",
    "    return rare_words\n",
    "\n",
    "def get_vectorizer(data,vectorizer=\"CountVectorizer\"):\n",
    "    \n",
    "    if vectorizer == \"TFIDF\":\n",
    "        tfidf = TfidfVectorizer()\n",
    "        tfidf.fit(data)\n",
    "        print(\"TF-IDF Vectorizer\")\n",
    "        return tfidf\n",
    "    cv = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "    cv.fit(data)\n",
    "    print(\"Count Vectorizer\")\n",
    "    return cv\n",
    "\n",
    "def vectorize_data(data,vectorizer=\"CountVectorizer\"):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "    \n",
    "    if vectorizer == \"TFIDF\":\n",
    "        tfidf = TfidfVectorizer()\n",
    "        tfidf.fit(data)\n",
    "        print(\"TF-IDF Vectorizer\")\n",
    "        return tfidf.transform(data).toarray()\n",
    "    cv = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "    cv.fit(data)\n",
    "    print(\"Count Vectorizer\")\n",
    "    return cv.transform(data).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols_stopwords_and_stem(data):\n",
    "    data = messy_text_to_df(data)\n",
    "    data[\"Data\"] = remove_non_words(data[\"Data\"],replacements)\n",
    "    data[\"Data\"] = remove_words(data[\"Data\"],stopwords)\n",
    "    data[\"Data\"] = stem(data[\"Data\"])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer\n"
     ]
    }
   ],
   "source": [
    "replacements = {\"can't\" : 'can not',\"shan't\":'shall not',\"won't\":'will not',\"'ve\" : \" have\", \"'d\" : \" would\", \"'m\" : \" am\", \"'ll\" : \" will\", \"'s\" : \"\", \"n't\" : \" not\",\"'re\" : \"are\"}\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.remove(\"not\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stopwords = set([\"can\",\"could\",\"would\",\"have\",\"go\",\"went\",\"zero\",\"one\",\"two\",\"three\",\"four\",\"five\",\"six\",\"seven\",\"eight\",\"nine\",\"ten\"]) | set(stopwords)\n",
    "\n",
    "train = remove_symbols_stopwords_and_stem(train)\n",
    "test = remove_symbols_stopwords_and_stem(test)\n",
    "\n",
    "rare_words = find_rare_words(train[\"Data\"])\n",
    "\n",
    "train[\"Data\"] = remove_words(train[\"Data\"],rare_words)\n",
    "test[\"Data\"] = remove_words(test[\"Data\"],rare_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stune even non this sound track beauti it pain...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the best soundtrack ever anyth i read lot revi...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amaz this soundtrack favorit music time hand t...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>excel soundtrack i truli like soundtrack i enj...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rememb pull your jaw off the floor after hear ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Data     Label\n",
       "0  stune even non this sound track beauti it pain...  Positive\n",
       "1  the best soundtrack ever anyth i read lot revi...  Positive\n",
       "2  amaz this soundtrack favorit music time hand t...  Positive\n",
       "3  excel soundtrack i truli like soundtrack i enj...  Positive\n",
       "4  rememb pull your jaw off the floor after hear ...  Positive"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer\n"
     ]
    }
   ],
   "source": [
    "vectoriser = get_vectorizer(train[\"Data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(vectoriser.transform(train[\"Data\"]).toarray(),train[\"Label\"])\n",
    "test[\"Prediction\"] = model.predict(vectoriser.transform(test[\"Data\"]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Models/sentiment_analysis_naive_bayes_model.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-25dfdd13d360>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/Models/sentiment_analysis_naive_bayes_model.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0mNumpyPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_filename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m             \u001b[0mNumpyPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Models/sentiment_analysis_naive_bayes_model.pkl'"
     ]
    }
   ],
   "source": [
    "joblib.dump(model,\"sentiment_analysis_naive_bayes_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained.\n",
      "F1 Score :  0.8211788211788211 \n",
      "Accuracy :  0.821\n"
     ]
    }
   ],
   "source": [
    "F1_Score = sklearn.metrics.f1_score(np.array(test[\"Label\"])==\"Positive\",test[\"Prediction\"]==\"Positive\")\n",
    "Accuracy = sklearn.metrics.accuracy_score(test[\"Label\"],test[\"Prediction\"])\n",
    "print(\"Model trained.\\n\"\"F1 Score : \",F1_Score,\"\\nAccuracy : \",Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
