{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = open(\"/Users/sougata-8718/Downloads/amazon_reviews_train.csv\",\"r+\")\n",
    "test = open(\"/Users/sougata-8718/Downloads/amazon_reviews_test.csv\",\"r+\")\n",
    "train = train.read()\n",
    "test = test.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.core._multiarray_umath'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.core._multiarray_umath'"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.core._multiarray_umath'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.core._multiarray_umath'"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.core._multiarray_umath'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.core._multiarray_umath'"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.core._multiarray_umath'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.core._multiarray_umath'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn\n",
    "import string\n",
    "import textblob\n",
    "from nltk.stem.snowball import SnowballStemmer,PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def messy_text_to_df(text):\n",
    "    documents = text.split(\"\\n\")\n",
    "    df = pd.DataFrame()\n",
    "    data = []\n",
    "    labels = []\n",
    "    for document in documents:\n",
    "        labels.append(document.split(\"\\t\",1)[0])\n",
    "        text = document.split('\\t')[1]\n",
    "        data.append(text)\n",
    "    labels = np.array(labels)\n",
    "    labels[np.where(labels=='__label__2')] = \"Positive\"\n",
    "    labels[np.where(labels=='__label__1')] = \"Negative\"\n",
    "    df[\"Data\"] = data\n",
    "    df[\"Label\"] = labels\n",
    "    \n",
    "    return df\n",
    "\n",
    "def remove_punctuation_and_numbers(text,replacements):\n",
    "    for key,value in replacements.items():\n",
    "        text = text.replace(key,value)\n",
    "    text = text.translate(str.maketrans('','',';\"#$%&\\'()*+/<=>?@[\\\\]^_`{|}~0123456789')).translate(str.maketrans('!.-:,','     '))\n",
    "    return text\n",
    "def remove_non_words(data,replacements):\n",
    "    res = data.apply(lambda x: remove_punctuation_and_numbers(x,replacements))\n",
    "    return res\n",
    "\n",
    "\n",
    "def remove_words_single(string,words_to_be_removed):\n",
    "    words = nltk.word_tokenize(string)\n",
    "    filtered_words = []\n",
    "    for i in range(len(words)):\n",
    "        if words[i] not in words_to_be_removed:\n",
    "            filtered_words.append(words[i])\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def remove_words(data,words_to_be_removed):\n",
    "    res = data.apply(lambda x : remove_words_single(x,words_to_be_removed))\n",
    "    return res\n",
    "    \n",
    "    for text,label in documents:\n",
    "        labels.append(document.split(\"\\t\",1)[0])\n",
    "        text = document.split('\\t')[1]\n",
    "        for key,value in replacements.items():\n",
    "            text = text.replace(key,value)\n",
    "            text = text.translate(str.maketrans('','',';\"#$%&\\'()*+/<=>?@[\\\\]^_`{|}~0123456789')).translate(str.maketrans('!.-:,','     '))\n",
    "        words = nltk.word_tokenize(text)\n",
    "        filtered_words = []\n",
    "        for i in range(len(words)):\n",
    "            if words[i] not in words_to_be_removed:\n",
    "                filtered_words.append(stemmer.stem(words[i]))\n",
    "        \n",
    "        res = ' '.join(filtered_words)\n",
    "        data.append(res)\n",
    "    labels = np.array(labels)\n",
    "    labels[np.where(labels=='__label__2')] = \"Positive\"\n",
    "    labels[np.where(labels=='__label__1')] = \"Negative\"\n",
    "    return data,labels\n",
    "\n",
    "def stem_single_string(string,nltkstemmer):\n",
    "    words = nltk.word_tokenize(string)\n",
    "    stemmed_list = []\n",
    "    for word in words:\n",
    "        stemmed_list.append(nltkstemmer.stem(word))\n",
    "    return ' '.join(stemmed_list)\n",
    "    \n",
    "\n",
    "def stem(data):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    res = data.apply(lambda x : stem_single_string(x,stemmer))\n",
    "    return res\n",
    "\n",
    "def find_rare_words(data,max_frequency=4):\n",
    "    \n",
    "    vectoriser = get_vectorizer(data)\n",
    "    \n",
    "    \n",
    "    temp = ' '.join(data)\n",
    "    frequencies = (nltk.FreqDist(nltk.word_tokenize(temp)))\n",
    "    \n",
    "    fs = np.array(frequencies.most_common())\n",
    "    fs = pd.DataFrame(fs)\n",
    "    fs.columns = [\"word\",\"count\"]\n",
    "    fs[\"freq\"] = fs[\"count\"].astype(int)\n",
    "    fs = fs.drop(\"count\",axis=1)\n",
    "    \n",
    "    rare_words = list(fs[fs[\"freq\"]<=max_frequency][\"word\"])\n",
    "    \n",
    "    return rare_words\n",
    "\n",
    "def get_vectorizer(data,vectorizer=\"CountVectorizer\"):\n",
    "    \n",
    "    if vectorizer == \"TFIDF\":\n",
    "        tfidf = TfidfVectorizer()\n",
    "        tfidf.fit(data)\n",
    "        print(\"TF-IDF Vectorizer\")\n",
    "        return tfidf\n",
    "    cv = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "    cv.fit(data)\n",
    "    print(\"Count Vectorizer\")\n",
    "    return cv\n",
    "\n",
    "def vectorize_data(data,vectorizer=\"CountVectorizer\"):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "    \n",
    "    if vectorizer == \"TFIDF\":\n",
    "        tfidf = TfidfVectorizer()\n",
    "        tfidf.fit(data)\n",
    "        print(\"TF-IDF Vectorizer\")\n",
    "        return tfidf.transform(data).toarray()\n",
    "    cv = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "    cv.fit(data)\n",
    "    print(\"Count Vectorizer\")\n",
    "    return cv.transform(data).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols_stopwords_and_stem(data):\n",
    "    data = messy_text_to_df(data)\n",
    "    data[\"Data\"] = remove_non_words(data[\"Data\"],replacements)\n",
    "    data[\"Data\"] = remove_words(data[\"Data\"],stopwords)\n",
    "    data[\"Data\"] = stem(data[\"Data\"])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer\n"
     ]
    }
   ],
   "source": [
    "replacements = {\"can't\" : 'can not',\"shan't\":'shall not',\"won't\":'will not',\"'ve\" : \" have\", \"'d\" : \" would\", \"'m\" : \" am\", \"'ll\" : \" will\", \"'s\" : \"\", \"n't\" : \" not\",\"'re\" : \"are\"}\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.remove(\"not\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stopwords = set([\"can\",\"could\",\"would\",\"have\",\"go\",\"went\",\"zero\",\"one\",\"two\",\"three\",\"four\",\"five\",\"six\",\"seven\",\"eight\",\"nine\",\"ten\"]) | set(stopwords)\n",
    "\n",
    "train = remove_symbols_stopwords_and_stem(train)\n",
    "test = remove_symbols_stopwords_and_stem(test)\n",
    "\n",
    "rare_words = find_rare_words(train[\"Data\"])\n",
    "\n",
    "train[\"Data\"] = remove_words(train[\"Data\"],rare_words)\n",
    "test[\"Data\"] = remove_words(test[\"Data\"],rare_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stune even non this sound track beauti it pain...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the best soundtrack ever anyth i read lot revi...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amaz this soundtrack favorit music time hand t...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>excel soundtrack i truli like soundtrack i enj...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rememb pull your jaw off the floor after hear ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>absolut masterpiec i quit sure actual take tim...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>buyer bewar this self publish book want know r...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>glorious stori i love wick saint the stori ama...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a five star book i finish read wick saint i fe...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>wick saint this easi read book made want keep ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>the worst a complet wast time error poor gramm...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>great book this great book i not put not read ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>great read i thought book brilliant yet realis...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>oh pleas i guess romanc novel lover not discer...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>aw beyond belief i feel i write keep other was...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>do not tri fool us fake review it obvious glow...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>a romant zen basebal comedi when hear folk say...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>fashion compress stock after i dvt doctor requ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ultrash thigh high excel product howev difficu...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>size recomend size chart not real size much sm...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>men ultrash this model may ok type i activ get...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>delici mix i thought funni i bought product wi...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>anoth abysm digit copi rather scratch drop ran...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>a fascin insight life modern japanes teen i th...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>like album thought i heard song thought listen...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>problem charg smaller aaa i charger year it ch...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>work not advertis i bought charger instruct sa...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>disappoint i read review made purchas disappoi...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>oh dear i excit find book muslim volum not liv...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>base review i bought i glad i this vcrdvd earl...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>book tell stori i love way author brought stor...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>i bias amaz book a must modern american art co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>nelli return wave st loui rapper nelli hit air...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>crocodil creek wild anim lunchbox i love croco...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>cute lunchbox love wild anim lunchbox it like ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>caught without word this album breath fresh ai...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>great effort simpli better band around today d...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>well craft music pleas not categor band attemp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>weird stuff dredg differ anyon i heard yet i n...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>best album i have ever heard period buy album ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>best album i heard on first listen catch witho...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>great cd i definit recommend catch without arm...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>an album i not get enough when i first time bl...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>author show vision jeff book our brown eye boy...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>heart awesom i absolut ador heart not heard al...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>heart capitol debut smash hit in heart made ca...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>heart beat heart still beat year yes hit magic...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>with help mtv heart back bigger ever heart las...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>best album femal lead group i love cd i never ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>life rocker had vinyl back these girl know roc...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>love this this thing freakin awesom you stand ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>better last last good this stori brother secon...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>veri good read i like book much better last bo...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>sword is fav favorit book it book read read wi...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>good kind corni good book kind corni i kid rea...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>this realli someth special i studi music class...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>sonic element i listen techno year sinc i live...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>fun swing jam session this nice record swing b...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>love da stuff i not pick guitar without tri pl...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>one who appreci good guitar jazz guitarist her...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Data     Label\n",
       "0    stune even non this sound track beauti it pain...  Positive\n",
       "1    the best soundtrack ever anyth i read lot revi...  Positive\n",
       "2    amaz this soundtrack favorit music time hand t...  Positive\n",
       "3    excel soundtrack i truli like soundtrack i enj...  Positive\n",
       "4    rememb pull your jaw off the floor after hear ...  Positive\n",
       "5    absolut masterpiec i quit sure actual take tim...  Positive\n",
       "6    buyer bewar this self publish book want know r...  Negative\n",
       "7    glorious stori i love wick saint the stori ama...  Positive\n",
       "8    a five star book i finish read wick saint i fe...  Positive\n",
       "9    wick saint this easi read book made want keep ...  Positive\n",
       "10   the worst a complet wast time error poor gramm...  Negative\n",
       "11   great book this great book i not put not read ...  Positive\n",
       "12   great read i thought book brilliant yet realis...  Positive\n",
       "13   oh pleas i guess romanc novel lover not discer...  Negative\n",
       "14   aw beyond belief i feel i write keep other was...  Negative\n",
       "15   do not tri fool us fake review it obvious glow...  Negative\n",
       "16   a romant zen basebal comedi when hear folk say...  Positive\n",
       "17   fashion compress stock after i dvt doctor requ...  Positive\n",
       "18   ultrash thigh high excel product howev difficu...  Positive\n",
       "19   size recomend size chart not real size much sm...  Negative\n",
       "20   men ultrash this model may ok type i activ get...  Negative\n",
       "21   delici mix i thought funni i bought product wi...  Positive\n",
       "22   anoth abysm digit copi rather scratch drop ran...  Negative\n",
       "23   a fascin insight life modern japanes teen i th...  Positive\n",
       "24   like album thought i heard song thought listen...  Positive\n",
       "25   problem charg smaller aaa i charger year it ch...  Negative\n",
       "26   work not advertis i bought charger instruct sa...  Negative\n",
       "27   disappoint i read review made purchas disappoi...  Negative\n",
       "28   oh dear i excit find book muslim volum not liv...  Negative\n",
       "29   base review i bought i glad i this vcrdvd earl...  Positive\n",
       "..                                                 ...       ...\n",
       "970  book tell stori i love way author brought stor...  Positive\n",
       "971  i bias amaz book a must modern american art co...  Positive\n",
       "972  nelli return wave st loui rapper nelli hit air...  Positive\n",
       "973  crocodil creek wild anim lunchbox i love croco...  Positive\n",
       "974  cute lunchbox love wild anim lunchbox it like ...  Positive\n",
       "975  caught without word this album breath fresh ai...  Positive\n",
       "976  great effort simpli better band around today d...  Positive\n",
       "977  well craft music pleas not categor band attemp...  Positive\n",
       "978  weird stuff dredg differ anyon i heard yet i n...  Positive\n",
       "979  best album i have ever heard period buy album ...  Positive\n",
       "980  best album i heard on first listen catch witho...  Positive\n",
       "981  great cd i definit recommend catch without arm...  Positive\n",
       "982  an album i not get enough when i first time bl...  Positive\n",
       "983  author show vision jeff book our brown eye boy...  Positive\n",
       "984  heart awesom i absolut ador heart not heard al...  Positive\n",
       "985  heart capitol debut smash hit in heart made ca...  Positive\n",
       "986  heart beat heart still beat year yes hit magic...  Positive\n",
       "987  with help mtv heart back bigger ever heart las...  Positive\n",
       "988  best album femal lead group i love cd i never ...  Positive\n",
       "989  life rocker had vinyl back these girl know roc...  Positive\n",
       "990  love this this thing freakin awesom you stand ...  Positive\n",
       "991  better last last good this stori brother secon...  Positive\n",
       "992  veri good read i like book much better last bo...  Positive\n",
       "993  sword is fav favorit book it book read read wi...  Positive\n",
       "994  good kind corni good book kind corni i kid rea...  Positive\n",
       "995  this realli someth special i studi music class...  Positive\n",
       "996  sonic element i listen techno year sinc i live...  Positive\n",
       "997  fun swing jam session this nice record swing b...  Positive\n",
       "998  love da stuff i not pick guitar without tri pl...  Positive\n",
       "999  one who appreci good guitar jazz guitarist her...  Positive\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer\n"
     ]
    }
   ],
   "source": [
    "vectoriser = get_vectorizer(train[\"Data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(vectoriser.transform(train[\"Data\"]).toarray(),train[\"Label\"])\n",
    "test[\"Prediction\"] = model.predict(vectoriser.transform(test[\"Data\"]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score :  0.8211788211788211 Accuracy :  0.821\n"
     ]
    }
   ],
   "source": [
    "F1_Score = sklearn.metrics.f1_score(np.array(test[\"Label\"])==\"Positive\",test[\"Prediction\"]==\"Positive\")\n",
    "Accuracy = sklearn.metrics.accuracy_score(test[\"Label\"],test[\"Prediction\"])\n",
    "print(\"F1 Score : \",F1_Score,\"Accuracy : \",Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
